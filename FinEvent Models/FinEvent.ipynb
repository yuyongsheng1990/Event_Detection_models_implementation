{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b839b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsettings:\\n    matplotlib==3.5.1\\n    networkx==2.6.3\\n    numpy==1.22.0\\n    pandas==1.3.5\\n    scikit-learn==1.0.1\\n    scipy==1.7.3\\n    torch==1.10.0\\n    torch-cluster==1.5.9\\n    torch-geometric==2.0.2\\n    torch-scatter==2.0.9\\n    torch-sparse==0.6.12\\n    torch-spline-conv==1.2.1\\n    tqdm==4.62.3\\n    dgl==0.4.3\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.10.6，复现FinEvent Model\n",
    "paper: from Reinforced, Incremental and Cross-lingual Event Detection From Social Messages\n",
    "github address: https://github.com/RingBDStack/FinEvent\n",
    "'''\n",
    "'''\n",
    "settings:\n",
    "    matplotlib==3.5.1\n",
    "    networkx==2.6.3\n",
    "    numpy==1.22.0\n",
    "    pandas==1.3.5\n",
    "    scikit-learn==1.0.1\n",
    "    scipy==1.7.3\n",
    "    torch==1.10.0\n",
    "    torch-cluster==1.5.9\n",
    "    torch-geometric==2.0.2\n",
    "    torch-scatter==2.0.9\n",
    "    torch-sparse==0.6.12\n",
    "    torch-spline-conv==1.2.1\n",
    "    tqdm==4.62.3\n",
    "    dgl==0.4.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cec4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6e88b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\GNN_Event_Detection_models\\\\FinEvent Models'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6960a",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04377446",
   "metadata": {},
   "source": [
    "## layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4ffe139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import Linear, BatchNorm1d, Sequential, ModuleList, ReLU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8f76901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    '''\n",
    "    adopt this module when using mini-batch\n",
    "    '''\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, heads) -> None:\n",
    "        super(GAT, self).__init__()\n",
    "        self.GAT1 = GATConv(in_channels=in_dim, out_channels=hid_dim, heads=heads, add_self_loops=False)\n",
    "        self.GAT2 = GATConv(in_channels=hid_dim*heads, out_channels=out_dim, add_self_loops=False)\n",
    "        self.layers = ModuleList([self.GAT1, self.GAT2])\n",
    "        self.norm = BatchNorm1d(heads * hid_dim)\n",
    "    \n",
    "    def forward(self, x, adjs, device):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            # x: Tensor, edge_index: Tensor\n",
    "            x, edge_index = x.to(device), edge_index.to(device)\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first\n",
    "            x = self.layers[i]((x,x_target), edge_index)\n",
    "            if i == 0:\n",
    "                x = self.norm(x)  # 归一化操作，防止梯度散射\n",
    "                x = F.elu(x)  # 非线性激活函数elu\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            del edge_index\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a32de874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intra_AGG(nn.Module):  # intra-aggregation\n",
    "    def __init__(self, GAT_args):\n",
    "        super(Intra_AGG, self).__init__()\n",
    "        in_dim, hid_dim, out_dim, heads = GAT_args\n",
    "        self.gnn = GAT(in_dim, hid_dim, out_dim, heads)\n",
    "    \n",
    "    def forward(self, x, adjs, device):\n",
    "        x = self.gnn(x, adjs, device)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ef8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inter_AGG(nn.Module):  # inter-aggregation\n",
    "    def __init__(self, mlp_args=None):\n",
    "        super(Inter_AGG, self).__init__()\n",
    "        if mlp_args is not None:\n",
    "            hid_dim, out_dim = mlp_args\n",
    "            self.mlp = nn.Sequential(\n",
    "                        Linear(hid_dim, hid_dim),\n",
    "                        BatchNorm1d(hid_dim),\n",
    "                        ReLU(inplace=True),\n",
    "                        Dropout(),\n",
    "                        Linear(hid_dim, out_dim),\n",
    "                        )\n",
    "    def forward(self, features, thresholds, inter_opt):\n",
    "        batch_size = features[0].size(0)\n",
    "        features = torch.transpose(features, dim0=0, dim1=1)\n",
    "        if inter_opt == 'cat_wo_avg':\n",
    "            features = features.reshape(batch_size, -1)\n",
    "        elif inter_opt == 'cat_w_avg':\n",
    "            # weighted average and concatenate\n",
    "            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n",
    "        elif inter_opt == 'cat_w_avg_mlp':\n",
    "            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n",
    "            features = self.mlp(features)\n",
    "        elif inter_opt == 'cat_wo_avg_mlp':\n",
    "            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n",
    "            features = self.mlp(features)\n",
    "        elif inter_opt == 'add_wo_avg':\n",
    "            features = features.sum(dim=1)\n",
    "        elif inter_opt == 'add_w_avg':\n",
    "            features = torch.mul(features, thresholds).sum(dim=1)\n",
    "        return features\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f92709",
   "metadata": {},
   "source": [
    "## TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02e50f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef756619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies an average on seq, of shape(nodes, features)\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "    def forward(self, seq):\n",
    "        return torch.mean(seq, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18a5185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):  # 鉴别器\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 1)  # 双向现行变换x1*A*x2\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "    \n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, m.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)  # 权值初始化方法，均分分布\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = torch.unsqueeze(c, 0)\n",
    "        c_x = c_x.expand_as(h_pl)  # torch.randn(size*)生成size维数组；expand是扩展到size_new数组；expand_as是扩展到像y的数组\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "        logits = torch.cat((sc_1, sc_2), 0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdb0c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineTripletLoss(nn.Module):\n",
    "    '''\n",
    "    Online Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels\n",
    "    Triplets are generated using triplet_selector objects that take embeddings and targets and return indices of triplets\n",
    "    '''\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(OnlineTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "    \n",
    "    def forward(self, embeddings, target):\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "        # if embeddings.is_cuda():\n",
    "        #     triplets = triplets.cuda()\n",
    "        ap_distances = (embeddings[triplets[:,0]] - embeddings[triplets[:,1]]).pow(2).sum(1) # .pow(.5)\n",
    "        an_distances = (embeddings[triplets[:,0]] - embeddings[triplets[:,2]]).pow(2).sum(1) # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "        \n",
    "        return losses.mean(), len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3d4f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(vectors):\n",
    "    distance_matrix=-2*vectors.mm(torch.t(vectors))+vectors.pow(2).sum(dim=1).view(1,-1)+vectors.pow(2).sum(dim=1).view(1,-1)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f125162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSelector:\n",
    "    '''\n",
    "    Implementation should return indices of anchors, positive and negative samples\n",
    "    return np array of shape [N_triplets * 3]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        raise NotImplementedError  # 如果这个方法没有被子类重写，但是调用了，就会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7683fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionNegativeTripletSelector(TripletSelector):\n",
    "    '''\n",
    "    For each positive pair, takes the hardes negative sample (with the greatest triplet loss value) to create a triplet\n",
    "    Margin should match the margin userd in triplet loss.\n",
    "    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n",
    "    and return a negative index for that pair\n",
    "    '''\n",
    "    def __init__(self, margin, negative_selection_fn, cpu=True):\n",
    "        super(FunctionNegativeTripletSelector, self).__init__()\n",
    "        self.cpu = cpu\n",
    "        self.margin = margin\n",
    "        self.negative_selection_fn = negative_selection_fn\n",
    "    \n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        if self.cpu:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "        distance_matrix = distance_matrix.cpu()\n",
    "        \n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        \n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2)) # all anchor-positive pairs\n",
    "            anchor_positives = np.array(anchor_positives)\n",
    "            \n",
    "            ap_distances = distance_matrix[anchor_positives[:,0], anchor_positives[:,1]]\n",
    "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
    "                loss_values = ap_distance - distance_matrix[\n",
    "                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
    "                loss_values = loss_values.data.cpu().numpy()\n",
    "                hard_negative = self.negative_selection_fn(loss_values)\n",
    "                if hard_negative is not None:\n",
    "                    hard_negative = negative_indices[hard_negative]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "        \n",
    "        if len(triplets) == 0:\n",
    "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
    "        \n",
    "        triplets = np.array(triplets)\n",
    "        return torch.LongTensor(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d22a306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hard_negative(loss_values):\n",
    "    hard_negatives = np.where(loss > 0)[0]\n",
    "    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "daebf6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardest_negative(loss_values):\n",
    "    hard_negative = np.argmax(loss_values)\n",
    "    return hard_negative if loss_values[hard_negative] > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ebc6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HardestNegativeTripletSelector(margin, cpu=False):\n",
    "    return FunctionNegativeTripletSelector(margin=margin, negative_selection_fn=hardest_negative, cpu=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6d97c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomNegativeTripletSelector(margin, cpu=False):\n",
    "    return FunctionNegativeTripletSelector(margin=margin, negative_selection_fn=random_hard_negative, cpu=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10751e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9853]],\n",
       "\n",
       "        [[-0.6914]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a0ce20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9285, -0.4395,  1.1670],\n",
       "         [-1.4341, -0.8047,  0.6164],\n",
       "         [ 0.4023, -0.7834, -1.9064]],\n",
       "\n",
       "        [[ 1.3818,  0.0470,  1.3903],\n",
       "         [ 1.1434,  0.7092, -0.8992],\n",
       "         [-0.5288, -0.4024, -1.4602]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988fdcd",
   "metadata": {},
   "source": [
    "## neighborRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dfce7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.functional import Tensor\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54e23d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_node_dist(multi_r_data, features, save_path=None):\n",
    "    '''\n",
    "    This is used to culculate the similarity between node and \n",
    "    its neighbors in advance in order to avoid the repetitive computation.\n",
    "    Args:\n",
    "        multi_r_data ([type]): [description]\n",
    "        features ([type]): [description]\n",
    "        save_path ([type], optional): [description]. Defaults to None.\n",
    "    '''\n",
    "    relation_config: Dict[str, Dict[int, Any]] = {}\n",
    "    for relation_id, r_data in enumerate(multi_r_data):\n",
    "        node_config: Dict[int, Any] = {}\n",
    "        r_data: Tensor\n",
    "        unique_nodes = r_data[1].unique()\n",
    "        num_nodes = unique_nodes.size(0)\n",
    "        for node in range(num_nodes):\n",
    "            # get neighbors' index\n",
    "            neighbors_idx = torch.where(r_data[1]==node)[0]\n",
    "            # get neghbors\n",
    "            neighbors = r_data[0, neighbors_idx]\n",
    "            num_neighbors = neighbors.size(0)\n",
    "            neighbors_features = features[neighbors, :]\n",
    "            target_features = features[node, :]\n",
    "            # calculate enclidean distance with broadcast\n",
    "            dist: Tensor = torch.norm(neighbors_features - target_features, p=2, dim=1)  # torch.norm求a列维度(dim指定)的2(p指定)范数(长度)\n",
    "            # smaller is better and we use 'top p' in our paper\n",
    "            # (threshold * num_neighbors) see RL_neighbor_filter for details\n",
    "            sorted_neighbors, sorted_index = dist.sort(descending=False)\n",
    "            node_config[node] = {'neighbors_idx': neighbors_idx,\n",
    "                                'sorted_neighbors': sorted_neighbors,\n",
    "                                'sorted_index': sorted_index,\n",
    "                                'num_neighbors': num_neighbors}\n",
    "        relation_config['relation_%d' % relation_id] = node_config\n",
    "    if save_path is not None:\n",
    "        save_path = os.path.join(save_path, 'relation_config.npy')\n",
    "        np.save(save_path, relation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe4bee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_neighbor_filter(multi_r_data, RL_thtesholds, load_path):\n",
    "    load_path = os.path.join(load_path, 'relation_config.npy')\n",
    "    relation_config = np.load(load_path, allow_pickle=True)\n",
    "    relation_config = relation_config.tolist()\n",
    "    relations = list(relation_config.keys())\n",
    "    multi_remain_data = []\n",
    "    \n",
    "    for i in range(len(relations)):\n",
    "        edge_index: Tensor = multi_r_data[i]\n",
    "        unique_nodes = edge_index[1].unique()\n",
    "        num_nodes = unique_nodes.size(0)\n",
    "        remain_node_index = torch.tensor([])\n",
    "        for node in range(num_nodes):\n",
    "            # extract config\n",
    "            neighbors_idx = relation_config[relations[i]][node]['neighbors_idx']\n",
    "            num_neighbors = relation_config[relations[i]][node]['num_neighbors']\n",
    "            sorted_neighbors = relation_config[relations[i]][node]['sorted_neighbors']\n",
    "            sorted_index = relation_config[relation[i]][node]['sorted_index']\n",
    "            \n",
    "            if num_neighbors < 5:\n",
    "                remain_node_index = torch.cat((remain_node_index, neighbors_idx))\n",
    "                continue  # add limitations\n",
    "            \n",
    "            threshold = float(RL_thtesholds[i])\n",
    "            \n",
    "            num_kept_neighbors_idx = neighbors_idx[sorted_index[:num_kept_neighbors]]\n",
    "            filtered_neighbors_idx = neighbors_idx[sorted_index[:num_kept_neighbors]]\n",
    "            remain_node_index = torch.cat((remain_node_index, filtered_neighbors_idx))\n",
    "        remain_node_index = remain_node_index.type('torch.LongTensor')\n",
    "        edge_index = edge_index[:, remain_node_index]\n",
    "        multi_remain_data.append(edge_index)\n",
    "    \n",
    "    return multi_remain_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498a445",
   "metadata": {},
   "source": [
    "# MarGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48b2889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import Tensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b68b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarGNN(nn.Module):\n",
    "    def __init__(self, GNN_args, num_relations, inter_opt, is_shared=False):\n",
    "        super(MarGNN, self).__init__()\n",
    "        \n",
    "        self.num_relations = num_relations\n",
    "        self.inter_opt = inter_opt\n",
    "        self.is_shared = is_shared\n",
    "        if not self.is_shared:\n",
    "            self.intra_aggs = torch.nn.ModuleList([Intra_AGG(GNN_args) for _ in range(self.num_relations)])\n",
    "        else:\n",
    "            self.intra_aggs = Intra_AGG(GNN_args) # shared parameters\n",
    "        \n",
    "        if self.inter_opt == 'cat_w_avg_mlp' or 'cat_wo_avg_mlp':\n",
    "            in_dim, hid_dim, out_dim, heads = GNN_args\n",
    "            mlp_args = self.num_relations * out_dim, out_dim\n",
    "        else:\n",
    "            self.inter_agg = Inter_AGG()\n",
    "    \n",
    "    def forward(self, x, adjs, n_ids, device, RL_thresholds):\n",
    "        # RL_threshold: tensor([[.5], [.5], [.5]])\n",
    "        if RL_thresholds is None:\n",
    "            RL_thresholds = torch.FloatTensor([[1.], [1.], [1.]])\n",
    "        if not isinstance(RL_thresholds, Tensor):\n",
    "            RL_thresholds = torch.FloatTensor(RL_thresholds)\n",
    "        RL_thresholds = RL_thresholds.to(device)\n",
    "        \n",
    "        features = []\n",
    "        for i in range(self.num_relations):\n",
    "            if not self.is_shared:\n",
    "                # print('Intra Aggregation of relation %d' % i)\n",
    "                features.append(self.intra_aggs[i](x[n_ids[i]], adjs[i], device))\n",
    "            else:\n",
    "                # shared parameters\n",
    "                # print('Shared Intra Aggregation ...')\n",
    "                features.append(self.intra_aggs(x[n_ids[i]], adjs[i], device))\n",
    "        \n",
    "        features = torch.stack(features, dim=0)\n",
    "        features = self.inter_agg(features, RL_thresholds, self.inter_opt)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27c2f2",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40212d7c",
   "metadata": {},
   "source": [
    "## step 1: generate initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101b1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis file generates the initial message features (please see Figure 1(b) and Section 3.2 of the paper for more details).\\nTo leverage the semantics in the data, we generate document feature for each message,\\nwhich is calculated as an average of the pre-trained word embeddings of all the words in the message\\nWe use the word embeddings pre-trained by en_core_web_lg, while other options, \\nsuch as word embeddings pre-trained by BERT, are also applicable.\\nTo leverage the temporal information in the data, we generate temporal feature for each message,\\nwhich is calculated by encoding the times-tamps: we convert each timestamp to OLE date, \\nwhose fractional and integral components form a 2-d vector.\\nThe initial feature of a message is the concatenation of its document feature and temporal feature.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the initial features for the messages\n",
    "'''\n",
    "This file generates the initial message features (please see Figure 1(b) and Section 3.2 of the paper for more details).\n",
    "To leverage the semantics in the data, we generate document feature for each message,\n",
    "which is calculated as an average of the pre-trained word embeddings of all the words in the message\n",
    "We use the word embeddings pre-trained by en_core_web_lg, while other options, \n",
    "such as word embeddings pre-trained by BERT, are also applicable.\n",
    "To leverage the temporal information in the data, we generate temporal feature for each message,\n",
    "which is calculated by encoding the times-tamps: we convert each timestamp to OLE date, \n",
    "whose fractional and integral components form a 2-d vector.\n",
    "The initial feature of a message is the concatenation of its document feature and temporal feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8000e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7794e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '../datasets/Twitter/'\n",
    "save_path = '../datasets/Twitter/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79eebee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m p_part1 \u001b[38;5;241m=\u001b[39m load_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m68841_tweets_multiclasses_filtered_0722_part1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m p_part2 \u001b[38;5;241m=\u001b[39m load_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m68841_tweets_multiclasses_filtered_0722_part2.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df_np_part1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_part1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow_pickle, Allow loading pickled object arrays stored in npy files\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_np_part2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(p_part2, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m df_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((df_np_part1, df_np_part2),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy'"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "p_part1 = load_path + '68841_tweets_multiclasses_filtered_0722_part1.npy'\n",
    "p_part2 = load_path + '68841_tweets_multiclasses_filtered_0722_part2.npy'\n",
    "df_np_part1 = np.load(p_part1, allow_pickle=True)  # allow_pickle, Allow loading pickled object arrays stored in npy files\n",
    "df_np_part2 = np.load(p_part2, allow_pickle=True)\n",
    "df_np = np.concatenate((df_np_part1, df_np_part2),axis=0)\n",
    "print('loaded data.')\n",
    "df = pd.DataFrame(data=df_np, columns=['event_id','tweet_id','text','user_id','created_at','user_loc','place_type',\n",
    "                                      'place_full_name','place_country_code','hashtags','user_mentions','image_urls',\n",
    "                                      'entities','words','filtered_words','sampled_words'])\n",
    "print('Data converted to dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings of all the documents in the dataframe\n",
    "# the embeddings of each document is an average of the pre-trained embeddings of all the words in it\n",
    "def documents_to_features(df):\n",
    "    nlp = en_core_web_lg.load()\n",
    "    features = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector).values\n",
    "    return np.stack(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57876cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode one times-tamp\n",
    "# t_str: a string of format '2012-10-11 07:19:34'\n",
    "def extract_time_feature(t_str):\n",
    "    t = datetime.fromisoformat(str(t_str)) # 分别返回年月日时分秒列表\n",
    "    OLE_TIME_ZERO = datetime(1899, 12, 30)\n",
    "    delta = t - OLE_TIME_ZERO\n",
    "    return [(float(delta.days)/10000.), (float(delta.seconds)/86400)] # 86400 seconds in day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed571763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the times-tamps of all the messages in the dateframe\n",
    "def df_to_t_features(df):\n",
    "    t_features = np.asarray([extract_time_feature(t_str) for t_str in df['created_at']])\n",
    "    return t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = documents_to_features(df)\n",
    "print('Document features generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = df_to_t_features(df)\n",
    "print('Time features generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.concatenate((d_features, t_features),axis=1)\n",
    "print('Concatenated document features and time features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy', combined_featurs)\n",
    "print('Initial features saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.load(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n",
    "print('Initial features loaded.')\n",
    "print(combined_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52eb60c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1899, 12, 30, 0, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime(1899, 12, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6daf11b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2012, 10, 11, 7, 19, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromisoformat('2012-10-11 07:19:34')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec3b46",
   "metadata": {},
   "source": [
    "## step 2: custom message graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a07ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct incremental message graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0a1fb",
   "metadata": {},
   "source": [
    "## construct_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512a2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nKPGNN:\\n    This file splits the Twitter dataset into 21 message blocks (please see Section 4.3 of the paper for more details), \\n    use the message blocks to construct heterogeneous social graphs (please see Figure 1(a) and Section 3.2 of the paper for more details) \\n    and maps them into homogeneous message graphs (Figure 1(c)).\\n    Note that:\\n    # 1) We adopt the Latest Message Strategy (which is the most efficient and gives the strongest performance. See Section 4.4 of the paper for more details) here, \\n    # as a consequence, each message graph only contains the messages of the date and all previous messages are removed from the graph;\\n    # To switch to the All Message Strategy or the Relevant Message Strategy, replace 'G = construct_graph_from_df(incr_df)' with 'G = construct_graph_from_df(incr_df, G)' inside construct_incremental_dataset_0922().\\n    # 2) For test purpose, when calling construct_incremental_dataset_0922(), set test=True, and the message blocks, as well as the resulted message graphs each will contain 100 messages.\\n    # To use all the messages, set test=False, and the number of messages in the message blocks will follow Table. 4 of the paper.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "KPGNN:\n",
    "    This file splits the Twitter dataset into 21 message blocks (please see Section 4.3 of the paper for more details), \n",
    "    use the message blocks to construct heterogeneous social graphs (please see Figure 1(a) and Section 3.2 of the paper for more details) \n",
    "    and maps them into homogeneous message graphs (Figure 1(c)).\n",
    "    Note that:\n",
    "    # 1) We adopt the Latest Message Strategy (which is the most efficient and gives the strongest performance. See Section 4.4 of the paper for more details) here, \n",
    "    # as a consequence, each message graph only contains the messages of the date and all previous messages are removed from the graph;\n",
    "    # To switch to the All Message Strategy or the Relevant Message Strategy, replace 'G = construct_graph_from_df(incr_df)' with 'G = construct_graph_from_df(incr_df, G)' inside construct_incremental_dataset_0922().\n",
    "    # 2) For test purpose, when calling construct_incremental_dataset_0922(), set test=True, and the message blocks, as well as the resulted message graphs each will contain 100 messages.\n",
    "    # To use all the messages, set test=False, and the number of messages in the message blocks will follow Table. 4 of the paper.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b2e59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6c99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a heterogeneous graph using tweet ids, user_ids, entities and rare(sampled) words(4 modalities模态)\n",
    "# if G is not None then insert new nodes to G\n",
    "def construct_graph_from_df(df, G=None):\n",
    "    if G is None:\n",
    "        G = nx.Graph()  # 创建无向图\n",
    "    for _, row in df.iterrows():\n",
    "        tid = 't' + str(row['tweet_id'])\n",
    "        G.add_node(tid) # 一次添加一个节点，字符串作为节点id\n",
    "        G.nodes[tid]['tweet_id'] = True # right-hand side value is irrelevant for the lookup\n",
    "        \n",
    "        user_ids = row['user_mentions']\n",
    "        user_ids.append(row['user_id'])\n",
    "        user_ids = ['u_' + str(each) for each in user_ids]\n",
    "        G.add_nodes_from(user_ids) # 添加多个节点\n",
    "        for each in user_ids:\n",
    "            G.nodes[each]['user_id'] = True \n",
    "        \n",
    "        entities = row['entities']\n",
    "#         words = ['e_' + each for each in entities]\n",
    "        G.add_nodes_from(entities)\n",
    "        for each in entities:\n",
    "            G.nodes[each]['entities'] = True\n",
    "        \n",
    "        words = row['sampled_words']\n",
    "        words = ['w_' + each for each in words]\n",
    "        G.add_nodes_from(words)\n",
    "        for each in words:\n",
    "            G.nodes[each]['word'] = True\n",
    "        \n",
    "        edges =[]\n",
    "        edges += [(tid, each) for each in user_ids]\n",
    "        edges += [(tid, each) for each in entities]\n",
    "        edges += [(tid, each) for each in words]\n",
    "        G.add_edges_from(edges) # 同时添加多条边\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767a10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a heterogeneous social graph G to a homogeneous message graph following eq. 1 of the paper, \n",
    "# and store the sparse binary adjacency matrix of the homogeneous message graph.\n",
    "def to_dgl_graph_v3(G, save_path=None):\n",
    "    message = ''\n",
    "    print('Start converting heterogeneous networks graph to homogeneous dgl graph.')\n",
    "    message += 'Start converting heterogeneous networks graph to homogeneous dgl graph.\\n'\n",
    "    all_start = time()\n",
    "    \n",
    "    print('\\tGetting a list of all nodes ...')\n",
    "    message += '\\tGetting a list of all nodes ...\\n'\n",
    "    start = time()\n",
    "    all_nodes = list(G.nodes)\n",
    "    mins = (time() -start) /60\n",
    "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\tGetting adjacency matrix ...')\n",
    "    message += '\\tGetting adjacency matrix ...\\n'\n",
    "    start = time()\n",
    "    A = nx.to_numpy_matrix(G) # Returns the graph adjacency matrix as a Numpy matrix\n",
    "    mins = (time() - start)/ 60\n",
    "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # compute commuting matrics\n",
    "    print('\\tGetting lists of nodes of various types ...')\n",
    "    message += '\\tGetting lists of nodes of various types ...\\n'\n",
    "    start = time()\n",
    "    tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys()) # get_node_attributes return node and its attributes\n",
    "    userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n",
    "    word_nodes = list(nx.get_node_attributes(G, 'word').keys())\n",
    "    entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n",
    "    del G\n",
    "    mins = (time() - start)/ 60\n",
    "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\tConverting node lists to index lists ...')\n",
    "    message += '\\tConverting node lists to index lists ...\\n'\n",
    "    start = time()\n",
    "    # fine the index of target nodes in the list of all nodes\n",
    "    indices_tid = [all_nodes.index(x) for x in tid_nodes]\n",
    "    indices_userid = [all_nodes.index for x in userid_nodes]\n",
    "    indices_word = [all_nodes.index(x) for x in word_nodes]\n",
    "    indices_entity = [all_nodes.index(x) for x in entity_nodes]\n",
    "    del tid_nodes\n",
    "    del userid_nodes\n",
    "    del word_nodes\n",
    "    del entity_nodes\n",
    "    mins = (time() -start)/60\n",
    "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # ----------------tweet-user-tweet------------------\n",
    "    print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n",
    "    print('\\t\\t\\tStart constructing tweet-user matrix ...')\n",
    "    message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user matrix ...\\n'\n",
    "    start = time()\n",
    "    w_tid_userid = A[np.ix_(indices_tid, indices_userid)]  #生成一个open mash数组\n",
    "    # return a N(indiced_tid)*N(indices_userid) matrix, representing the weight of edges between tid and userid\n",
    "    mins = (time() - start)/60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # convert to scipy sparse matrix\n",
    "    print('\\t\\t\\tConverting to sparse matrix ...')\n",
    "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
    "    start = time()\n",
    "    s_w_tid_userid = sparse.csr_matrix(w_tid_userid) # matrix compression\n",
    "    del w_tid_userid\n",
    "    mins = (time() - start)/ 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tTransposing ...')\n",
    "    message += '\\t\\t\\tTransposing ...\\n'\n",
    "    start = time()\n",
    "    s_w_userid_tid = s_w_tid_userid.transpose()\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n",
    "    message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n",
    "    start = time()\n",
    "    s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid # homogeneous message graph\n",
    "    mins = (time() - start)/ 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tSaving ...')\n",
    "    message += '\\t\\t\\tSaving ...\\n'\n",
    "    start = time()\n",
    "    if save_path is not None:\n",
    "        sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n",
    "        print('sparse binary userid commuting matrix saved.')\n",
    "        del s_m_tid_userid_tid\n",
    "    del s_w_tid_userid\n",
    "    del s_w_userid_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # ----------tweet-ent-tweet-----------------\n",
    "    print('\\tStart constructing tweet-ent-tweet conmuting matrix ...')\n",
    "    print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n",
    "    message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ...\\n'\n",
    "    start = time()\n",
    "    w_tid_entity = A[np.ix_(indices_tid, indices_entity)]\n",
    "    mins = (time() - start) / 60\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # convert to scipy sparse matrix\n",
    "    print('\\t\\t\\tConverting to sparse matrix ...')\n",
    "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
    "    start = time()\n",
    "    s_w_tid_entity = sparse.csr_matrix(w_tid_entity)\n",
    "    del w_tid_entity\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed : ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tTransposing ...')\n",
    "    message += '\\t\\t\\tTransposing ...\\n'\n",
    "    start = time()\n",
    "    s_w_entity_tid = s_w_tid_entity.transpose()\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n",
    "    message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n",
    "    start = time()\n",
    "    s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tSaving ...')\n",
    "    message += '\\t\\t\\tSaving ...\\n'\n",
    "    start = time()\n",
    "    if save_path is not None:\n",
    "        sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n",
    "        print('Sparse binary entity commuting matrix saved.')\n",
    "        del s_m_tid_entity_tid\n",
    "    del s_w_tid_entity\n",
    "    del s_w_entity_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # --------tweet-word-tweet------------------\n",
    "    print('\\tStart constructing tweet-word-tweet commuting matrix ...')\n",
    "    print('\\t\\t\\tStart constructing tweet-word matrix ...')\n",
    "    message +='\\tStart constructing tweet-wrod-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-word matrix ...'\n",
    "    start = time()\n",
    "    w_tid_word = A[np.ix_(indices_tid, indices_word)]\n",
    "    del A\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # convert to scipy sparse matrix\n",
    "    print('\\t\\t\\tConverting to Sparse matrix ...')\n",
    "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
    "    start = time()\n",
    "    s_w_tid_word = sparse.csr_matrix(w_tid_word)\n",
    "    del w_tid_word\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tTransposing ...')\n",
    "    message += '\\t\\t\\tTransposing ...\\n'\n",
    "    start = time()\n",
    "    s_w_word_tid = s_w_tid_word.transpose()\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tCalculating tweet-word * word-tweet ...')\n",
    "    message += '\\t\\t\\tCalculating tweet-word * word-tweet ...\\n'\n",
    "    start = time()\n",
    "    s_m_tid_word_tid = s_w_tid_word * s_w_word_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    print('\\t\\t\\tSaving ...')\n",
    "    message += '\\t\\t\\tSaving ...\\n'\n",
    "    start = time()\n",
    "    if save_path is not None:\n",
    "        sparse.save_npz(save_path + \"s_m_tid_owrd_tid.npz\", s_m_tid_word_tid)\n",
    "        print(\"Sparse binary word commuting matrix saved.\")\n",
    "        del s_m_tid_word_tid\n",
    "    del s_w_tid_word\n",
    "    del s_w_word_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    # -----------compute tweet-tweet adjacency matrix --------\n",
    "    print('\\tComputing tweet-tweet adjacency matrix ...')\n",
    "    message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n",
    "    start = time()\n",
    "    if save_path is not None:\n",
    "        s_m_tid_userid_tid = sparse.load_npz(save_path + 's_m_tid_userid_tid.npz')\n",
    "        print(\"Sparse binary userid commuting matrix loaded.\")\n",
    "        s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n",
    "        print(\"Sparse binary entity commuting matrix loaded.\")\n",
    "        s_m_tid_word_tid = sparse.load_npz(save_path + \"s_m_tid_word_tid.npz\")\n",
    "        print(\"Sparse binary word commuting matrix loaded.\")\n",
    "        \n",
    "    s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n",
    "    del s_m_tid_userid_tid\n",
    "    del s_m_tid_entity_tid\n",
    "    s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_word_tid).astype('bool')  # confirm the connect between tweets\n",
    "    del s_m_tid_word_tid\n",
    "    del s_A_tid_tid\n",
    "    mins = (time() - start) / 60\n",
    "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
    "    message += '\\t\\t\\tDone. Time elapsed: '\n",
    "    message += str(mins)\n",
    "    message += ' mins\\n'\n",
    "    all_mins = (time() - all_start) / 60\n",
    "    print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n",
    "    message += '\\tOver all time elapsed: '\n",
    "    message += str(all_mins)\n",
    "    message += ' mins\\n'\n",
    "    \n",
    "    if save_path is not None:\n",
    "        sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n",
    "        print(\"Sparse binary adjacency matrix saved.\")\n",
    "        s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n",
    "        print(\"Sparse binary adjacency matrix loaded.\")\n",
    "        \n",
    "    # create correspoinding dgl graph\n",
    "    G = dgl.DGLGraph(s_bool_A_tid_tid)\n",
    "    print('We have %d nodes.' % G.number_of_nodes())\n",
    "    print('We have %d edges' % G.number_of_edges())\n",
    "    print()\n",
    "    message += 'We have '\n",
    "    message += str(G.number_of_nodes())\n",
    "    message += ' nodes.'\n",
    "    message += 'We have '\n",
    "    message += str(G.number_of_edges())\n",
    "    message += ' edges.\\n'\n",
    "    \n",
    "    return all_mins, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddba2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Twitter dataset by date into 21 message blocks, use the message blocks to construct heterogeneous social graphs,\n",
    "# and maps them into homogeneous message graphs. \n",
    "# Note that:\n",
    "# 1) We adopt the Latest Message Strategy (which is the most efficient and gives the strongest performance. See Section 4.4 of the paper for more details) here, \n",
    "# as a consequence, each message graph only contains the messages of the date and all previous messages are removed from the graph;\n",
    "# To switch to the All Message Strategy or the Relevant Message Strategy, replace 'G = construct_graph_from_df(incr_df)' with 'G = construct_graph_from_df(incr_df, G)'.\n",
    "# 2) For test purpose, set test=True, and the message blocks, as well as the resulted message graphs each will contain 100 messages.\n",
    "# To use all the messages, set test=False, and the number of messages in the message blocks will follow Table. 4 of the paper.\n",
    "def construct_incremental_dataset_0922(df, save_path, features, test=True):\n",
    "    # If test equals true, construct the initial graph using test_ini_size_tweets\n",
    "    # and increment the graph by test_incr_size tweets each day\n",
    "    test_ini_size = 500\n",
    "    test_incr_size = 100\n",
    "    \n",
    "    # save data splits for training/validate/test mask generation\n",
    "    # data_split = []\n",
    "    # save time spent for the heterogeneous -> homogeneous conversion of each graph\n",
    "    all_graph_mins = []\n",
    "    message = ''\n",
    "    # extract distingct dates\n",
    "    distinct_dates = df.date.unique() # 所有unique的date\n",
    "    print('Number of distinct dates: ', len(distinct_dates))\n",
    "    print()\n",
    "    message += 'Number of distinct dates: '\n",
    "    message += str(len(distinct_dates))\n",
    "    message += '\\n'\n",
    "    \n",
    "    # split data by dates and construct graphs\n",
    "    # first week -> initial graph (20254 tweets)\n",
    "    print('Start constructing initial graph ...')\n",
    "    message += '\\nStart constructing initial graph ...\\n'\n",
    "    ini_df = df\n",
    "    G = construct_graph_from_df(ini_df)\n",
    "    path = save_path + '0/'\n",
    "    os.mkdir(path)  # 创建目录\n",
    "    graph_mins, graph_message = to_dgl_graph_v3(G, save_path=path)  # convert a heterogeneous social graph to a homogeneous message graph\n",
    "    message += graph_message\n",
    "    print('Initial graph saved')\n",
    "    message += 'Initial graph saved\\n'\n",
    "    # record the totoal number of tweets\n",
    "    all_graph_mins.append(graph_mins)\n",
    "    # extract and save the labels of corresponding tweets\n",
    "    y = ini_df['event_id'].values\n",
    "    y = [int(each) for each in y]\n",
    "    np.save(path + 'labels.npy', np.asarray(y))  # ndarray对象，实际只创建一个指针\n",
    "    print('Labels saved.')\n",
    "    message += 'Labels saved.\\n'\n",
    "    # extract and sve the features of corresponding tweets\n",
    "    indices = ini_df['index'].values.tolist()\n",
    "    x = features[indices, :]\n",
    "    np.save(path + 'features.npy', x)\n",
    "    print('Features saved.')\n",
    "    message += 'Features saved. \\n\\n'\n",
    "    \n",
    "    return message, all_graph_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83c4197",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m p_part1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m p_part2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part2.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m df_np_part1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_part1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow_pickle: 可选，布尔值，允许使用 Python pickles 保存对象数组，Python 中的 pickle 用于在保存到磁盘文件或从磁盘文件读取之前，对对象进行序列化和反序列化。\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_np_part2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(p_part2, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m df_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((df_np_part1, df_np_part2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy'"
     ]
    }
   ],
   "source": [
    "save_path = './incremental_test/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "# load data (68841 tweets, multiclasses filtered)\n",
    "p_part1 = '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part1.npy'\n",
    "p_part2 = '../datasets/Twitter/68841_tweets_multiclasses_filtered_0722_part2.npy'\n",
    "df_np_part1 = np.load(p_part1, allow_pickle=True)  # allow_pickle: 可选，布尔值，允许使用 Python pickles 保存对象数组，Python 中的 pickle 用于在保存到磁盘文件或从磁盘文件读取之前，对对象进行序列化和反序列化。\n",
    "df_np_part2 = np.load(p_part2, allow_pickle=True)\n",
    "df_np = np.concatenate((df_np_part1, df_np_part2), axis=0)\n",
    "print('Data loaded.')\n",
    "df = pd.DataFrame(data=df_np, columns=['event_id', 'tweet_id', 'text', 'user_id', 'created_at', 'user_loc',\n",
    "                                      'place_type', 'place_full_name', 'place_country_code', 'hashtags',\n",
    "                                      'user_mentions', 'image_urls', 'entities', 'words', 'filtered_words', 'sampled_words'])\n",
    "print('Data converted to dataframe.')\n",
    "\n",
    "# sort data by time\n",
    "df = df.sort_values(by='created_at').reset_index()\n",
    "# append data\n",
    "df['data'] = [d.date() for d in df['created_at']]\n",
    "# load features\n",
    "# the dimension of feature is 300 in this dataset\n",
    "f = np.load('../datasets/Twitter/features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n",
    "# generate test graphs, features, and labels\n",
    "message, all_graph_mins = construct_incremental_dataset_0922(df, save_path, f, True)\n",
    "with open(save_path + 'node_edge_statistics.txt', 'w') as text_file:\n",
    "    text_file.write(message)\n",
    "np.save(save_path + 'all_graph_min.npy', np.asarray(all_graph_mins))\n",
    "print('Time spent on heterogeneous -> homogeneous graph conversions: ', all_graph_mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "259d866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [5],\n",
       "        [7],\n",
       "        [2]]),\n",
       " array([[0, 3, 1, 2]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ix_([1,5,7,2],[0,3,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f1186",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5753eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility，功能\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 交集\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def run_kmeans(extract_features, extract_labels, indices, isoPath=None):\n",
    "    # extract the features and labels of the test tweets\n",
    "    if isoPath is not None:\n",
    "        # Remove isolated points\n",
    "        temp = torch.load(isoPath)\n",
    "        temp = temp.cpu().detach().numpy()  # detach()阻断反向传播，返回值为tensor；numpy()将tensor转换为numpy\n",
    "        non_isolated_index = list(np.where(temp != 1)[0]) # np.where返回符合条件元素的索引index\n",
    "        indices = intersection(indices, non_isolated_index)\n",
    "    # Extract labels\n",
    "    extract_labels = extract_labels.cpu().numpy()\n",
    "    labels_true = extract_labels[indices]\n",
    "    \n",
    "    # Extrac features\n",
    "    X = extract_features.cpu().detach().numpy()\n",
    "    assert labels_true.shape[0] == X.shape[0]  # assert断言，在判断式false时触发异常\n",
    "    n_test_tweets = X.shape[0]  # 100\n",
    "    \n",
    "    # Get the total number of classes\n",
    "    n_classes = len(ser(labels_true.tolist()))  # unique()和nunique()不香吗？\n",
    "    \n",
    "    # k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    nmi = metrics.normalized_mutual_info_score(labels_true, labels)  # 计算归一化互信息\n",
    "    ami = metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "    ari = metrics.adjusted_mutual_info_score(labels_true, labels)  # 计算兰德系数\n",
    "    \n",
    "    # Return number of test tweets, number of classes covered by the test tweets, and KMeans clustering NMI\n",
    "    return n_test_tweets, n_classes, nmi, ami, ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0535e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(extract_features, extract_labels, indices, epoch, num_isolated_nodes, save_path,\n",
    "             is_validation=True, cluster_type='kmeans'):\n",
    "    message = ''\n",
    "    message += '\\nEpoch '\n",
    "    message += str(epoch)\n",
    "    message += '\\n'\n",
    "    \n",
    "    # with isolated nodes\n",
    "    if cluster_type == 'kmeans':\n",
    "        n_tweets, n_classes, nmi, ami, ari = run_kmeans(extract_features, extract_labels, indices)\n",
    "    elif cluster_type == 'dbscan':\n",
    "        pass\n",
    "    \n",
    "    if is_validation:\n",
    "        mode = 'validation'\n",
    "    else:\n",
    "        mode = 'test'\n",
    "    message += '\\tNumber of ' + mode + ' tweets: '\n",
    "    message += str(n_tweets)\n",
    "    message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n",
    "    message += str(n_classes)\n",
    "    message += '\\n\\t' + mode + ' NMI: '\n",
    "    message += str(nmi)\n",
    "    message += '\\n\\t' + mode + 'AMi: '\n",
    "    message += str(ami)\n",
    "    message += '\\n\\t' + mode + 'ARI'\n",
    "    message += str(ari)\n",
    "    if cluster_type == 'dbscan':\n",
    "        message += '\\n\\t' + mode + ' best_eps: '\n",
    "        message += '\\n\\t' + mode + ' best_min_Pts: '\n",
    "    \n",
    "    if num_isolated_nodes != 0:\n",
    "        # without isolated nodes\n",
    "        message += '\\n\\tWithout isolated nodes:'\n",
    "        n_tweets, n_classes, nmi, ami, ari = run_kmeans(extract_features, extract_labels, indices, \n",
    "                                                       save_path + '/isolated_nodes.pt')\n",
    "        message += '\\tNumber of ' + mode + 'tweets: '\n",
    "        message += str(n_tweets)\n",
    "        message += '\\n\\tNumber of classes covered by ' + mode + ' tweets'\n",
    "        message += str(n_classes)\n",
    "        message += '\\n\\t' + mode + 'NMI: '\n",
    "        message += str(nmi)\n",
    "        message += '\\n\\t' + mode + 'AMI: '\n",
    "        message += str(ami)\n",
    "        message += '\\n\\t' + mode + 'ARI: '\n",
    "        message += str(ari)\n",
    "    message += '\\n'\n",
    "    \n",
    "    with open(save_path + '/evaluate.txt', 'a') as f:\n",
    "        f.write(message)\n",
    "    print(message)\n",
    "    \n",
    "    np.save(save_path + '/%s_metric.npy' % mode, np.asarray([nmi, ami, ari]))\n",
    "    return nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58def6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMasks(length, data_split, train_i, i, validation_percent=0.2, save_path=None, remove_absolete=2):\n",
    "    '''    \n",
    "    Intro:\n",
    "    This function generates train and validation indices for initial/maintenance epochs and test indices for inference(prediction) epochs\n",
    "    If remove_obsolete mode 0 or 1:\n",
    "    For initial/maintenance epochs:\n",
    "    - The first (train_i + 1) blocks (blocks 0, ..., train_i) are used as training set (with explicit labels)\n",
    "    - Randomly sample validation_percent of the training indices as validation indices\n",
    "    For inference(prediction) epochs:\n",
    "    - The (i + 1)th block (block i) is used as test set\n",
    "    Note that other blocks (block train_i + 1, ..., i - 1) are also in the graph (without explicit labels, only their features and structural info are leveraged)\n",
    "    If remove_obsolete mode 2:\n",
    "    For initial/maintenance epochs:\n",
    "    - The (i + 1) = (train_i + 1)th block (block train_i = i) is used as training set (with explicit labels)\n",
    "    - Randomly sample validation_percent of the training indices as validation indices\n",
    "    For inference(prediction) epochs:\n",
    "    - The (i + 1)th block (block i) is used as test set\n",
    "    :param length: the length of label list\n",
    "    :param data_split: loaded splited data (generated in custom_message_graph.py)\n",
    "    :param train_i, i: flag, indicating for initial/maintenance stage if train_i == i and inference stage for others\n",
    "    :param validation_percent: the percent of validation data occupied in whole dataset\n",
    "    :param save_path: path to save data\n",
    "    :param num_indices_to_remove: number of indices ought to be removed\n",
    "    :returns train indices, validation indices or test indices\n",
    "    '''\n",
    "    # step1: verify total number of nodes\n",
    "    assert length == data_split[i] # 500\n",
    "    \n",
    "    # step2.0: if is in initial/maintenance epochs, generate train and validation indices\n",
    "    if train_i == i:\n",
    "        # step3: randomly shuffle the graph indices\n",
    "        train_indices = torch.randperm(length)  # 返回一个随机打散的0-n-1数组\n",
    "        # step4: get total number of validation indices\n",
    "        n_validation_sample = int(length * validation_percent)\n",
    "        # step5: sample n_validation_samples validation indices and use the rest as training indices\n",
    "        validation__indices = train_indices[:n_validation_samples]\n",
    "        train_indices = train_indices[n_validation_samples:]\n",
    "        # step6: save indices\n",
    "        if save_path is not None:\n",
    "            torch.save(train_indices, save_path + '/train_indices.pt')\n",
    "            torch.save(validation_indices, save_path + '/validation_indices.pt')\n",
    "        return train_indices, validation__indices\n",
    "    # step2.1: if is in inference(prediction) epochs, generate test indices\n",
    "    else:\n",
    "        test_indices = torch.arange(0, (data_split[i]), dtype=torch.long)\n",
    "        if save_path is not None:\n",
    "            torch.save(test_indices, save_path + '/test_indices.pt')\n",
    "        return test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fb08455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_offline_masks(length, validation_percent=0.2, test_percent=0.1):\n",
    "    test_length = int(length * test_percent)\n",
    "    valid_length = int(length * validation_percent)\n",
    "    train_length = length - valid_length - test_length\n",
    "    \n",
    "    samples = torch.randperm(length)\n",
    "    train_indices = samples[:train_length]\n",
    "    valid_indices = samples[train_length: train_length + valid_length]\n",
    "    test_indices = samples[train_length + valid_length:]\n",
    "    \n",
    "    return train_indices, valid_length, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6988d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(extracted_features, save_path):\n",
    "    torch.save(extracted_features, save_path + '/final_embeddings.pt')\n",
    "    print('extracted features saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e48b2eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2,43,2,42,12,454])[0]\n",
    "np.where(a<5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bada4",
   "metadata": {},
   "source": [
    "## gen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbddfacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.coo import coo_matrix\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy import sparse\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_sparse.tensor import SparseTensor\n",
    "# from .utils import generateMasks, gen_offline_masks，是指从utils.py文件中导入函数: generatemasks, gen_offline_masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7bdb196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_trans(datapath = 'incremental_0808/0/s_m_tid_userid_tid.npz'):\n",
    "    relation = sparse.load_npz(datapath)\n",
    "    all_edge_index = torch.tensor([], dtype=int)\n",
    "    for node in range(relation.shape[0]):\n",
    "        neighbor = torch.IntTensor(relation[node].toarray()).squeeze()  # IntTensor是torch定义的7中cpu tensor类型之一；\n",
    "                                                                        # squeeze对数据维度进行压缩，删除所有为1的维度\n",
    "        # del self_loop in advance\n",
    "        neighbor[node] = 0\n",
    "        neighbor_idx = neighbor.nonzero()  # 返回非零元素的索引\n",
    "        neighbor_sum = neighbor_idx.size(0)  # 表示第0维度的数据量\n",
    "        loop = torch.tensor(node).repeat(neighbor_sum, 1)  # repeat表示沿着指定的维度重复tensor的次数\n",
    "        edge_index_i_j = torch.cat((loop, neighbor_idx), dim=1).t()  # cat表示拼接；t表示对二维矩阵进行转置\n",
    "        self_loop = torch.tensor([[node], [node]])\n",
    "        all_edge_index = torch.cat((all_edge_index, edge_index_i_j, self_loop), dim=1)\n",
    "        del neighbor, neighbor_idx, loop, self_loop, edge_index_i_j\n",
    "    return all_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f15cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo_trans(datapath = 'incremental_0808/0/s_m_tid_userid_tid.npz'):\n",
    "    relation: csr_matrix = sparse.load_npz(datapath)\n",
    "    relation: coo_matrix = relation.tocoo()\n",
    "    sparse_edge_index = torch.LongTensor([relation.row, relation.col])  # sparse稀疏矩阵用三元组(row,col,data)来存储非零元素信息\n",
    "    return sparse_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "685fc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(loadpath, relation, mode):\n",
    "    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n",
    "    features = torch.FloatTensor(features)\n",
    "    print('features laoded')\n",
    "    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n",
    "    print('labels loaded')\n",
    "    labels = torch.LongTensor(labels)\n",
    "    relation_edge_index = coo_trans(os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation))\n",
    "    print('edge index laoded')\n",
    "    data = Data(x=features, edge_index=relation_edge_index, y=labels)\n",
    "    data_split = np.load(os.path.join(loadpath, 'data_split.npy'))\n",
    "    train_i, i = mode[0], mode[1]\n",
    "    if train_i == i:\n",
    "        data.train_mask, data.val_mask = generateMasks(len(labels), data_split, train_i, i)\n",
    "    else:\n",
    "        data.test_mask = generateMasks(len(labels), data_split, train_i, i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37bfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_homodataset(loadpath, mode, valid_percent=0.2):\n",
    "    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n",
    "    features = torch.FloatTensor(features)\n",
    "    print('features loaded')\n",
    "    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n",
    "    print('labels loaded')\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    data = Data(x=features, edge_index=None, y=labels)  # torch_geometric提供的图数据类型Data，x表示tensor矩阵，\n",
    "                                                        # 形状为[num_nodes, num_node_features]; edge_index表示coo格式的图的边关系，\n",
    "                                                        # 形状为[2, num_edge]\n",
    "    data_split = np.load(os.path.join(loadpath, 'data_split.npy'))\n",
    "    train_i, i = mode[0], mode[1]\n",
    "    if train_i == i:\n",
    "        data.train_mask, data.val_mask = generateMasks(len(labels), data_split, train_i, i, valid_percent)\n",
    "    else:\n",
    "        data.test_mask = generateMasks(len(labels), data_split, train_i, i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83696191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_offline_homodataset(loadpath, mode):\n",
    "    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n",
    "    features = torch.FloatTensor(features)\n",
    "    print('features loaded')\n",
    "    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n",
    "    print('labels loaded')\n",
    "    labels = torch.LongTensor(labels)\n",
    "    data = Data(x=features, edge_index=None, y=labels)\n",
    "    data.train_mask, data.val_mask, data.test_mask = generateMasks(len(labels))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "106be59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_relational_graph(loadpath, relations, mode):\n",
    "    multi_relation_edge_index = [torch.load(loadpath + '/' + str(mode[1]), '/edge_index_%s.pt' % relatio) for relation in relations]\n",
    "    print('sparse trans...')\n",
    "    print('edge index loaded')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2637c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_relational_graph(loadpath, relations, mode):\n",
    "    for relation in relations:\n",
    "        relation_edge_index = sparse_trans(os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation))\n",
    "        torch.save(relation_edge_index, loadpath + '/' + str(mode[1]) + '/edge_index_%s.pt' % relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ca133",
   "metadata": {},
   "source": [
    "## step 3: save edge index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "545c1a32",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'incremental_cross_English_68841\\\\0\\\\s_m_tid_entity_tid.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m relation_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m22\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43msave_multi_relational_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge index saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall edge index saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36msave_multi_relational_graph\u001b[1;34m(loadpath, relations, mode)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_multi_relational_graph\u001b[39m(loadpath, relations, mode):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m relation \u001b[38;5;129;01min\u001b[39;00m relations:\n\u001b[1;32m----> 3\u001b[0m         relation_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43msparse_trans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloadpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms_m_tid_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_tid.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(relation_edge_index, loadpath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(mode[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/edge_index_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m relation)\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36msparse_trans\u001b[1;34m(datapath)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msparse_trans\u001b[39m(datapath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincremental_0808/0/s_m_tid_userid_tid.npz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     relation \u001b[38;5;241m=\u001b[39m \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     all_edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(relation\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\sparse\\_matrix_io.py:123\u001b[0m, in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_npz\u001b[39m(file):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\" Load a sparse matrix from a file using ``.npz`` format.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m            [4, 0, 0]], dtype=int64)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mPICKLE_KWARGS) \u001b[38;5;28;01mas\u001b[39;00m loaded:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m             matrix_format \u001b[38;5;241m=\u001b[39m loaded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'incremental_cross_English_68841\\\\0\\\\s_m_tid_entity_tid.npz'"
     ]
    }
   ],
   "source": [
    "# acclerate the training process\n",
    "import torch\n",
    "\n",
    "data_path = 'incremental_cross_English_68841'\n",
    "relation_ids = ['entity', 'userid', 'word']\n",
    "for i in range(22):\n",
    "    save_multi_relational_graph(data_path, relation_ids, [0,i])\n",
    "    print('edge index saved')\n",
    "print('all edge index saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8235d17",
   "metadata": {},
   "source": [
    "## mysampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e9abf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import List, Optional, Tuple, NamedTuple, Union, Callable\n",
    "from scipy import sparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.loader import NeighborSampler, RandomNodeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca743db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampler(object):\n",
    "    def __init__(self, sampler) -> None:\n",
    "        super().__init__()\n",
    "        self.sampler = sampler\n",
    "    \n",
    "    def smaple(self, multi_relational_edge_index: List[Tensor], node_idx, sizes, batch_size):\n",
    "        if self.sampler == 'RL_sampler':\n",
    "            return self._RL_sample(multi_relational_edge_index, node_idx, sizes, batch_size)\n",
    "        elif self.sampler == 'randdom_sampler':\n",
    "            return self._random_sample(multi_relational_edge_index, node_idx, batch_size)\n",
    "        elif self.sampler == 'const_sampler':\n",
    "            return self._const_sample(multi_relational_edge_index, node_idx, batch_size)\n",
    "    \n",
    "    def _RL_sample(self, multi_relational_edge_index: List[Tensor], node_idx, sizes, batch_size):\n",
    "        outs = []\n",
    "        all_n_ids = []\n",
    "        for id, edge_index in enumerate(multi_relational_edge_index):  # 返回数据和数据下标\n",
    "            loader = NeighborSampler(edge_index=edge_index, sizes=sizes, node_idx=node_idx, return_e_id=False,\n",
    "                                    batch_size=batch_size, num_workers=0)\n",
    "            for id, (_, n_ids, adjs) in enumerate(loader):\n",
    "                outs.append(adjs)\n",
    "                all_n_ids.append(n_ids)\n",
    "            \n",
    "            assert id == 0\n",
    "        return outs, all_n_ids\n",
    "    \n",
    "    def _random_sample(self, multi_relational_edge_index: List[Tensor], node_idx, batch_size):\n",
    "        outs = []\n",
    "        all_n_ids = []\n",
    "        sizes = [random.randint(10,100), random.randint(10,50)]\n",
    "        for edge_index in multi_relational_edge_index:\n",
    "            loader = NeighborSampler(edge_index=edge_index, sizes=sizes, node_idx=node_idx, return_e_id=False,\n",
    "                                    batch_size=batch_size, num_workers=0)\n",
    "            for id, (_, n_ids, adjs) in enumerate(loader):\n",
    "                outs.append(adjs)\n",
    "                all_n_ids.append(n_ids)\n",
    "            assert id == 0\n",
    "        return outs, all_n_ids\n",
    "\n",
    "    def _const_sample(self, multi_relational_edge_index: List[Tensor], node_idx, batch_size):\n",
    "        outs = []\n",
    "        all_n_ids = []\n",
    "        sizes = [25, 15]\n",
    "        for edge_index in multi_relational_edge_index:\n",
    "            loader = NeighborSampler(edge_index=edge_index, sizes=sizes, node_idx=node_idx, return_e_id=False,\n",
    "                                    batch_size=batch_size, num_workers=0)\n",
    "            for id, (_, n_ids, adjs) in enumerate(loader):\n",
    "                outs.append(adjs)\n",
    "                all_n_ids.append(n_ids)\n",
    "            assert id == 0\n",
    "        return outs, all_n_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15beca",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d58c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "17692535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, outputs, target, loss):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def value(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def name(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fade06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccumulateAccuracy(Metric):\n",
    "    '''\n",
    "    works with classification model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def __call__(self, outputs, target, loss):\n",
    "        pred = outputs[0].data.max(1, keepdim=True)[1]\n",
    "        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n",
    "        self.total += target[0].size(0)\n",
    "        return self.value()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def value(self):\n",
    "        return 100 * float(self.correct) / self.total\n",
    "    \n",
    "    def name(self):\n",
    "        return 'Accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16733de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageNonzeroTripletsMetric(Metric):\n",
    "    '''\n",
    "    Counts average number of nonzero triplets found in minibatches\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "    \n",
    "    def __call__(self, outputs, target, loss):\n",
    "        self.values.append(loss[1])\n",
    "        return self.value()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.values = []\n",
    "    \n",
    "    def value(self):\n",
    "        return np.mean(self.values)\n",
    "    \n",
    "    def name(self):\n",
    "        return 'Average nonzero triplets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02c407",
   "metadata": {},
   "source": [
    "## stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4d0a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import gc  # garbage cleaning package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1c1b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinEvent():\n",
    "    def __init__(self, args) -> None:\n",
    "        # register args\n",
    "        self.args = args\n",
    "    \n",
    "    def inference(self,\n",
    "                 train_i, i,\n",
    "                 metrics,\n",
    "                 embedding_save_path,\n",
    "                 loss_fn,\n",
    "                 model: MarGNN,\n",
    "                 RL_thresholds=None,\n",
    "                 loss_fn_dgi=None):\n",
    "        # make dir for graph i\n",
    "        # ./incremental_0808//embeddings_0403005348/block_xxx\n",
    "        save_path_i = embedding_save_path + '/block_' + str(i)\n",
    "        if not os.path.isdir(save_path_i):\n",
    "            os.mkdir(save_path_i)\n",
    "        \n",
    "        # load data\n",
    "        relation_ids: List[str] = ['entity', 'userid', 'word']\n",
    "        homo_data = create_homodataset(self.args.data_path, [train_i, i], self.args.validation_percent)\n",
    "        multi_r_data = create_multi_relational_graph(self.args.data_path, relation_ids, [train_i, i])\n",
    "        num_relations = len(multi_r_data)\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() and self.args.use_cuda else 'cpu')\n",
    "        \n",
    "        # input ddimension (300 in our paper)\n",
    "        features = homo_data.x\n",
    "        feat_dim = features.size(1)\n",
    "        \n",
    "        # prepare graph configs for node filtering\n",
    "        if self.args.is_initial:\n",
    "            print('prepare node configures...')\n",
    "            pre_node_dist(multi_r_data, homo_data.x, save_path_i)\n",
    "            filter_path = save_path_i\n",
    "        else:\n",
    "            filter_path = self.args.data_path + str(i)\n",
    "        \n",
    "        if model is None:\n",
    "            assert 'Cannot fine pre-trained model'\n",
    "        \n",
    "        # directly predict\n",
    "        message = '\\n-----------------Directly predict on block' + str(i) + '-----------------\\n'\n",
    "        print(message)\n",
    "        print('RL Threshold using in this block:', RL_thresholds)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        test_indices, labels = homo_data.test_mask, homo_data.y\n",
    "        test_num_samples = test_indices.size(0)\n",
    "        \n",
    "        sampler = MySampler(self.args.sampler)\n",
    "        \n",
    "        # filter neighbor in advance to fit with neighbor sampling\n",
    "        filtered_multi_r_data = RL_neighbor_filter(multi_r_data, RL_thresholds, filter_path) if RL_thresholds is not None and \\\n",
    "                                self.args.sampler == 'RL_sampler' else multi_r_data\n",
    "        \n",
    "        # batch testing\n",
    "        extract_features = torch.FloatTensor([])\n",
    "        num_batches = int(test_num_samples / self.args.batch_size) + 1\n",
    "        with torch.no_grad():  # 在该模块下，所有计算得出的tensor的requires_grad都自动设置为False，不自动反向传播求导\n",
    "            for batch in range(num_batches):\n",
    "                start_batch = time.time()\n",
    "                \n",
    "                # split batch\n",
    "                i_start = self.args.batch_size * batch\n",
    "                i_end = min((batch + 1) * self.args.batch_size, test_num_samples)\n",
    "                batch_nodes = test_indices[i_start:i_end]\n",
    "                \n",
    "                # sampling neighbors of batch nodes\n",
    "                adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx= batch_nodes, sizes=[-1, -1], \n",
    "                                             batch_size= self.args.batch_size)\n",
    "                pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n",
    "                batch_seconds_spent = time.time() - start_batch\n",
    "                \n",
    "                # for we haven't shuffle the test indices(see utils.py)\n",
    "                # the output embeddings can be simply stacked together\n",
    "                extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n",
    "                \n",
    "                del pred\n",
    "                gc.collect()\n",
    "        \n",
    "        save_embeddings(extract_features, save_path_i)\n",
    "        test_nmi = evaluate(extract_features,\n",
    "                           labels,\n",
    "                           indices=test_indices,\n",
    "                           epoch=-1, # just for test\n",
    "                           num_isolated_nodes=0,\n",
    "                           save_path= save_path_i,\n",
    "                           is_validation= False,\n",
    "                           cluster_type= self.args.cluster_type)\n",
    "        del homo_data, multi_r_data, features, filtered_multi_r_data\n",
    "        torch.cuda.empty_cache()  # 释放显存\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # train on initial/maintenance graphs, t==0 or t % window_size == 0 in this paper\n",
    "    def initial_maintain(self,\n",
    "                        train_i, i,\n",
    "                        metrics,\n",
    "                        embedding_save_path,\n",
    "                        loss_fn,\n",
    "                        model=None,\n",
    "                        loss_fn_dgi=None):\n",
    "        '''\n",
    "        :param i:\n",
    "        :param data_split:\n",
    "        :param metrics:\n",
    "        :param embedding_save_path:\n",
    "        :param loss_fn:\n",
    "        :param model:\n",
    "        :param loss_fn_dgi:\n",
    "        :return:\n",
    "        '''\n",
    "        # make dir for graph i\n",
    "        # ./incremental_0808//embeddings_0403005348/block_xxx\n",
    "        save_path_i = embedding_save_path + '/block_' + str(i)\n",
    "        if not os.path.isdir(save_path_i):\n",
    "            os.mkdir(save_path_i)\n",
    "        \n",
    "        # load data\n",
    "        relation_ids: List[str] = ['entity', 'userid', 'word']\n",
    "        homo_data = create_homodataset(self.args.data_path, [train_i, i], self.args.validation_percent)\n",
    "        multi_r_data = create_multi_relational_graph(self.args.data_path, relation_ids, [train_i, i])\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() and self.args.use_cuda else 'cpu')\n",
    "        \n",
    "        # input dimension (300 in our paper)\n",
    "        num_dim = homo_data.x.size(0)\n",
    "        feat_dim = homo_data.x.size(1)\n",
    "        \n",
    "        # prepare graph configs for node filtering\n",
    "        if self.args.is_initial:\n",
    "            print('prepare node %configures...')\n",
    "            pre_node_dist(multi_r_data, homo_data.x, save_path_i)\n",
    "            filter_path = save_path_i\n",
    "        else:\n",
    "            filter_path = self.args.data_path + str(i)\n",
    "        \n",
    "        if model is None: # pre-training stage in our paper\n",
    "            # print('Pre-Train Stage')\n",
    "            model = MarGNN((feat_dim, self.args.hidden_dim, self.args.out_dim, self.args.heads),\n",
    "                          num_relations=num_relations, inter_opt=self.args.inter_opt, is_shared=self.args.is_shared)\n",
    "        \n",
    "        # define sampler\n",
    "        sampler = MySampler(self.args.sampler)\n",
    "        # load model to device\n",
    "        model.to(device)\n",
    "        \n",
    "        # initialize RL thresholds\n",
    "        # RL_threshold: [[.5], [.5], [.5]]\n",
    "        RL_thresholds = torch.FloatTensor(self.args.threshold_start0)\n",
    "        \n",
    "        # define optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.args.lr, weight_decay=1e-4)\n",
    "        \n",
    "        # record training log\n",
    "        message = '\\n------------- Start initial training / maintaining using block ' + str(i) + '----------\\n'\n",
    "        print(message)\n",
    "        with open(save_path_i + '/log.txt', 'a') as f:\n",
    "            f.write(message)\n",
    "        \n",
    "        # record the highest validation nmi ever got for early stopping\n",
    "        best_vali_nmi = 1e-9\n",
    "        best_epoch = 0\n",
    "        wait = 0\n",
    "        # record validation nmi of all epochs before early stop\n",
    "        all_vali_nmi = []\n",
    "        # recore the time spent in seconds on each batch of all training/maintaining epochs\n",
    "        seconds_train_batches = []\n",
    "        # record the time spent in mins on each epoch\n",
    "        mins_train_epochs = []\n",
    "        \n",
    "        # step13: start training\n",
    "        for epoch in range(self.args.n_epochs):\n",
    "            start_epoch = time.time()\n",
    "            losses = []\n",
    "            totoal_loss = 0.0\n",
    "            \n",
    "            for metric in metrics:\n",
    "                metic.reset()\n",
    "                \n",
    "            # Multi-Agent\n",
    "            \n",
    "            # filter neighbor in advance to fit with neighbor sampling\n",
    "            filtered_multi_r_data = RL_neighbor_filter(multi_r_data, RL_thresholds, filter_path) if epoch >= self.args.RL_start0 and \\\n",
    "                                     self.args.sampler == 'RL_sampler' else multi_r_data\n",
    "            model.train()\n",
    "            train_num_samples, valid_num_samples = homo_data.train_mask.size(0), homo_data.vali_mask.size(0)\n",
    "            all_num_samples = train_num_samples + valid_num_samples\n",
    "            \n",
    "            # batch training\n",
    "            num_batches = int(train_num_samples / self.args.batch_size) + 1\n",
    "            for batch in range(num_batches):\n",
    "                start_batch = time.time()\n",
    "                # split batch\n",
    "                i_start = self.args.batch_size * batch\n",
    "                i_end = min((batc + 1) * self.args.batch_size, train_num_samples)\n",
    "                batch_nodes = homo_data.train_mask[i_start:i_end]\n",
    "                batch_labels = homo_data.y[batch_nodes]\n",
    "                \n",
    "                # sampling neighobrs of batch nodes\n",
    "                adjs, n_ids = sampler.sample(filtered_multi_r_data, node_ids=batch_nodes, sizes=[-1,-1], batch_size=self.args.batch_size)\n",
    "                \n",
    "                pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n",
    "                \n",
    "                extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n",
    "                \n",
    "                del pred\n",
    "                gc.collect()\n",
    "            \n",
    "            # save_embeddings(extract_reatures, save_path_i)\n",
    "            # evaluate the model: conduct kMeans clustering on the validation and report NMI\n",
    "            validation_nmi = evaluate(extract_features[homo_data.vali_mask],\n",
    "                                     homo_data.y,\n",
    "                                     epoch=epoch,\n",
    "                                     num_isolated_nodes=0,\n",
    "                                     save_path=save_path_i,\n",
    "                                     is_validation=True,\n",
    "                                     cluster_type=self.args.cluster_type)\n",
    "            all_vali_nmi.append(validation_nmi)\n",
    "            \n",
    "            # step16: early stop\n",
    "            if validation_nmi > best_vali_nmi:\n",
    "                best_vali_nmi = validation_nmi\n",
    "                best_epoch = epoch\n",
    "                wait = 0\n",
    "                # save model\n",
    "                model_path = save_path_i + '/models'\n",
    "                if (epoch == 0) and (not os.path.isdir(model_path)):\n",
    "                    os.mkdir(model_path)\n",
    "                p = model_path + '/best.pt'\n",
    "                torch.save(model.state_dict(), p)  # 保存模型，OrderDict存储网络结构的名字和对应的参数\n",
    "                print('Best model saved after epoch ', str(epoch))\n",
    "            else:\n",
    "                wait += 1\n",
    "            if wait >= self.args.patience:\n",
    "                print('Saved all_mins_spent')\n",
    "                print('Early stopping at epoch ', str(epoch))\n",
    "                print('Best model was at epoch ', str(best_epoch))\n",
    "                break\n",
    "            # end one epoch\n",
    "        \n",
    "        # save all validation mi\n",
    "        np.save(save_path_i + '/all_vali_nmi.npy', np.asarray(all_vali_nmi))\n",
    "        # save time spent on epochs\n",
    "        np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n",
    "        print('Saved mins_train_epochs')\n",
    "        # save time spent on batches\n",
    "        np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n",
    "        print('Best model loaded.')\n",
    "        \n",
    "        del homo_data, multi_r_data\n",
    "        torch.cuda.empty_cache()  # 释放显存\n",
    "        \n",
    "        return model, RL_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba64d0f",
   "metadata": {},
   "source": [
    "# FinEvent Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 1. run utils/generate_initial_features.py to generate the initial features for the messages\n",
    "\n",
    "step 2. run utils/custom_message_graph.py to construct incremental message graphs. To construct small message graphs for test purpose, set test=True when calling construct_incremental_dataset_0922(). To use all the messages (see Appendix of the paper for a statistic of the number of messages in the graphs), set test=False.\n",
    "\n",
    "step 3. run utils/save_edge_index.py in advance to acclerate the training process.\n",
    "\n",
    "step 4. run main.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0aff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json  # 轻量级的数据交换格式，易于阅读和编写\n",
    "import argparse  # 用于更方便地进行超参数的保存和修改\n",
    "import torch\n",
    "\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccf6d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_register():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_epochs', default=50, type=int, help='Number of initial-training/maintenance-training epochs.')\n",
    "    parser.add_argument('--window_size', default=3, type=int, help='Maintain the model after predicting window_size blocks.')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='Early stop if performance did not improve in the last patience epochs.')\n",
    "    \n",
    "    parser.add_argument('--margin', default=3, type=float, help='Margin for computing triplet losses')\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='Learning rate')\n",
    "    parser.add_argument('--batch_size', default=100, type=int, help='Batch size (number of nodes sampled to compute triplet loss in each batch)')\n",
    "    \n",
    "    parser.add_argument('--hidden_dim', default=128, type=int, help='Hidden dimension')\n",
    "    parser.add_argument('--out_dim', default=64, type=int, help='Output dimension of tweet representations')\n",
    "    parser.add_argument('heads', default=4, type=int, help='Number of heads used in GAT')\n",
    "    parser.add_argument('--validation_percent', default=0.2, type=float, help='Percentage of validation nodes(tweets)')\n",
    "    parser.add_argument('--use_hardest_neg', dest='user_hardes_neg', default=False, action='store_true', \n",
    "                       help='If true, use hardest negative messages to form triplets. Otherwise use random ones')\n",
    "    parser.add_argument('--is_shared', default=False)\n",
    "    parser.add_argument('--inter_opt', default='cat_w_avg')\n",
    "    parser.add_argument('--is_initial', default=False)\n",
    "    parser.add_argument('--sampler', default='RL_sampler')\n",
    "    parser.add_argument('--cluster_type', default='kmeans', help='Types of clustering algorithms') # DBSCAN\n",
    "    \n",
    "    # RL-0\n",
    "    parser.add_argument('--threshod_start0', default=[[0.2],[0.2],[0.2]], type=float, \n",
    "                        help='The initial value of filter threshold for state1 or state3')\n",
    "    parser.add_argument('--RL_step0', default=0.02, type=float, help='The step size of RL for state1 or state3')\n",
    "    parser.add_argument('--RL_start0', default=0, type=int, help='The starting epoch of RL for state1 or state3')\n",
    "    \n",
    "    # RL-1\n",
    "    parser.add_argument('--eps_start', default=0.001, type=float, help='The initial value of the eps for state2')\n",
    "    parser.add_argument('--eps_step', default=0.02, type=float, help='The step size of eps for state2')\n",
    "    parser.add_argument('--min_Pts_start', default=2, type=int, help='The initial value of the min_Pts for state2')\n",
    "    parser.add_argument('--min_Pts_step', default=1, type=int, help='The step size of min_Pts for state2')\n",
    "    \n",
    "    # other arguments\n",
    "    parser.add_argument('--user_cuda', dest='use_cuda', default=True, action='store_true', help='Use cuda')\n",
    "    parser.add_argument('--data_path', default='./incremental_0502/', type=str, help='Path of features, labels and edges')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--mask_path', default=None, type=str, help='File path that contains the training, validation and test masks')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--log_interval', default=10, type=int, help='Log interval')\n",
    "    args = parser.parse_args()  # 解析参数\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67e60b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_epochs N_EPOCHS] [--window_size WINDOW_SIZE] [--patience PATIENCE]\n",
      "                             [--margin MARGIN] [--lr LR] [--batch_size BATCH_SIZE] [--hidden_dim HIDDEN_DIM]\n",
      "                             [--out_dim OUT_DIM] [--validation_percent VALIDATION_PERCENT] [--use_hardest_neg]\n",
      "                             [--is_shared IS_SHARED] [--inter_opt INTER_OPT] [--is_initial IS_INITIAL]\n",
      "                             [--sampler SAMPLER] [--cluster_type CLUSTER_TYPE] [--threshod_start0 THRESHOD_START0]\n",
      "                             [--RL_step0 RL_STEP0] [--RL_start0 RL_START0] [--eps_start EPS_START]\n",
      "                             [--eps_step EPS_STEP] [--min_Pts_start MIN_PTS_START] [--min_Pts_step MIN_PTS_STEP]\n",
      "                             [--user_cuda] [--data_path DATA_PATH] [--mask_path MASK_PATH]\n",
      "                             [--log_interval LOG_INTERVAL]\n",
      "                             heads\n",
      "ipykernel_launcher.py: error: argument heads: invalid int value: 'C:\\\\Users\\\\yysgz\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-179d08b2-6a7c-4674-b3ab-198f3dcbaecf.json'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2482\u001b[0m, in \u001b[0;36mArgumentParser._get_value\u001b[1;34m(self, action, arg_string)\u001b[0m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2482\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtype_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# ArgumentTypeErrors indicate errors\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'C:\\\\Users\\\\yysgz\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-179d08b2-6a7c-4674-b3ab-198f3dcbaecf.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:1857\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1857\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2069\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n\u001b[1;32m-> 2069\u001b[0m stop_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_positionals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;66;03m# if we didn't consume all the argument strings, there were extras\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2025\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_positionals\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   2024\u001b[0m     start_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m arg_count\n\u001b[1;32m-> 2025\u001b[0m     \u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[38;5;66;03m# slice off the Positionals that we just parsed and return the\u001b[39;00m\n\u001b[0;32m   2028\u001b[0m \u001b[38;5;66;03m# index at which the Positionals' string args stopped\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:1918\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.take_action\u001b[1;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[0;32m   1917\u001b[0m seen_actions\u001b[38;5;241m.\u001b[39madd(action)\n\u001b[1;32m-> 1918\u001b[0m argument_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument_strings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# error if this argument is not allowed with other previously\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# seen arguments, assuming that actions that use the default\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# value don't really count as \"present\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2449\u001b[0m, in \u001b[0;36mArgumentParser._get_values\u001b[1;34m(self, action, arg_strings)\u001b[0m\n\u001b[0;32m   2448\u001b[0m arg_string, \u001b[38;5;241m=\u001b[39m arg_strings\n\u001b[1;32m-> 2449\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_value(action, value)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2495\u001b[0m, in \u001b[0;36mArgumentParser._get_value\u001b[1;34m(self, action, arg_string)\u001b[0m\n\u001b[0;32m   2494\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid \u001b[39m\u001b[38;5;132;01m%(type)s\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m%(value)r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m args)\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;66;03m# return the converted value\u001b[39;00m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument heads: invalid int value: 'C:\\\\Users\\\\yysgz\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-179d08b2-6a7c-4674-b3ab-198f3dcbaecf.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# define args\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43margs_register\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# check CUDA\u001b[39;00m\n",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36margs_register\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--log_interval\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog interval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 解析参数\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:1824\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1824\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:1860\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1859\u001b[0m         err \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1860\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2581\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2580\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\argparse.py:2568\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2568\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1983\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   1981\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1982\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1983\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1986\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1987\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   1988\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:443\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    440\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    441\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    442\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 443\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m tb\n\u001b[1;32m-> 1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1009\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    858\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    862\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    863\u001b[0m ):\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    797\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(etype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m    798\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 799\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    803\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    848\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    849\u001b[0m options \u001b[38;5;241m=\u001b[39m stack_data\u001b[38;5;241m.\u001b[39mOptions(\n\u001b[0;32m    850\u001b[0m     before\u001b[38;5;241m=\u001b[39mbefore,\n\u001b[0;32m    851\u001b[0m     after\u001b[38;5;241m=\u001b[39mafter,\n\u001b[0;32m    852\u001b[0m     pygments_formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[0;32m    853\u001b[0m )\n\u001b[1;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[tb_offset:]\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\stack_data\\core.py:546\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[1;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_data\u001b[39m(\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m         collapse_repeated_frames: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrameInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, RepeatedFrames]]:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;66;03m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;66;03m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;66;03m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_frame(frame_or_tb):\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\stack_data\\utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame_or_tb:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m frame_or_tb\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     99\u001b[0m         frame_or_tb \u001b[38;5;241m=\u001b[39m frame_or_tb\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\stack_data\\utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[43massert_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracebackType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame_or_tb, (types\u001b[38;5;241m.\u001b[39mFrameType,))\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\stack_data\\utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[1;34m(condition, error)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    171\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAssertionError\u001b[39;00m(error)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # define args\n",
    "    args = args_register()\n",
    "    \n",
    "    # check CUDA\n",
    "    print('Using CUDA:', torch.cuda.is_available())\n",
    "    \n",
    "    # create working path\n",
    "    embedding_save_path = args.data_path + '/embeddings_' + strftime(\"%m%d%H%M%S\", localtime())\n",
    "    os.mkdir(embedding_save_path)\n",
    "    print('embedding save path: ', embedding_save_path)\n",
    "    \n",
    "    # record hyper-parameters\n",
    "    with open(embedding_save_path + '/args.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "    \n",
    "    print('Batch Size:', args.batch_size)\n",
    "    print('Intra Agg Mode:', args.is_shared)\n",
    "    print('Inter Agg Mode:', args.inter_opt)\n",
    "    print('Reserve node config?', args.is_initial)\n",
    "    \n",
    "    # load number of messages in each blocks\n",
    "    # e.g. data_split = [  500  ,   100, ...,  100]\n",
    "    #                    block_0  block_1    block_n\n",
    "    data_split = np.load(args.data_path + '/data_split.npy')\n",
    "    \n",
    "    # define loss function\n",
    "    # contrastive loss in our paper\n",
    "    if args.use_hardest_neg:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, HardestNegativeTripletSelector(args.margin))\n",
    "    else:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, RandomNegativeTripletSelector(args.margin))\n",
    "        \n",
    "    # define metrics\n",
    "    BCL_metrics = [AverageNonzeroTriplesMetric()]\n",
    "    \n",
    "    # define detection stage\n",
    "    Streaming = FinEvent(args)\n",
    "    \n",
    "    # pre-train stage: train on initial graph\n",
    "    train_i = 0\n",
    "    model, RL_thresholds = Streaming.initial_maintain(train_i = train_i,\n",
    "                                                     i = 0,\n",
    "                                                     metrics = BCL_metrics, \n",
    "                                                     embedding_save_path = embedding_save_path,\n",
    "                                                     loss_fn = loss_fn,\n",
    "                                                     model = None)\n",
    "    \n",
    "    # detection-maintenance stage: incremental training and detection\n",
    "    for i in range(1, data_split.shape[0]):\n",
    "        # infer every block\n",
    "        model = Streaming.inference(train_i=train_i,\n",
    "                                   i=i,\n",
    "                                   metrics=BCL_metrics,\n",
    "                                   embedding_save_path=embedding_save_path,\n",
    "                                   loss_fn=loss_fn,\n",
    "                                   model=model,\n",
    "                                   RL_thresholds=RL_thresholds)\n",
    "        \n",
    "        # maintenance in window size and desert the last block\n",
    "        if i % args.window_size == 0 and i != data_split.shape[0] -1:\n",
    "            model, RL_thresholds = Streaming.initial_maintain(train_i=train_i,\n",
    "                                                             i=i,\n",
    "                                                             metrics=BCL_metrics,\n",
    "                                                             embedding_save_path=embedding_save_path,\n",
    "                                                             loss_fn=loss_fn,\n",
    "                                                             model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427cbd0",
   "metadata": {},
   "source": [
    "# FinEvent Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 1. run utils/generate_initial_features.py to generate the initial features for the messages\n",
    "\n",
    "step 2. run utils/custom_message_graph.py to construct incremental message graphs. To construct small message graphs for test purpose, set test=True when calling construct_incremental_dataset_0922(). To use all the messages (see Appendix of the paper for a statistic of the number of messages in the graphs), set test=False.\n",
    "\n",
    "step 3. run utils/save_edge_index.py in advance to acclerate the training process.\n",
    "\n",
    "step 4. run offline.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "166a0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from time import localtime, strftime  # strftime() 函数用于格式化时间，返回以可读字符串表示的当地时间\n",
    "\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import time \n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa0aa778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_register():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_epochs', default=50, type=int, help='Number of initial-training/maintenance-training epochs.')\n",
    "    parser.add_argument('--window_size', default=3, type=int, help='Maintain the model after predicting window_size blocks.')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='Early stop if perfermance did not improve in the last patience epochs.')\n",
    "    parser.add_argument('--margin', default=3, type=float, help='Margin for computing triplet losses')\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='Learning rate')\n",
    "    \n",
    "    parser.add_argument('--batch_size', default=100, type=int, help='Batch size (number of nodes sampled to compute triplet loss in each batch)')\n",
    "    parser.add_argument('--hidden_dim', default=128, type=int, help='Hidden dimension')\n",
    "    parser.add_argument('--out_dim', default=64, type=int, help='Output dimension of tweet representation')\n",
    "    parser.add_argument('--heads', default=4, type=int, help='Number of heads used in GAT')\n",
    "    parser.add_argument('--validation_percent', default=0.2, type=float, help='Percentage of validation nodes(tweets)')\n",
    "    parser.add_argument('--use_hardest_neg', dest='use_hardest_neg', default=False, action='store_true', \n",
    "                        help='If true, use hardest negative messages to form triplets. Otherwise use random ones')\n",
    "    parser.add_argument('--is_shared', default=False)\n",
    "    parser.add_argument('--inter_opt', default='cat_w_avg')\n",
    "    parser.add_argument('--is_initial', default=False)\n",
    "    parser.add_argument('--sampler', default='RL_sampler')\n",
    "    parser.add_argument('--cluster_type', default='kmeans', help='Types of clustering algorithms') # DBSCAN\n",
    "    \n",
    "    # RL-0\n",
    "    parser.add_argument('--threshold_start0', default=[[0.2],[0.2],[0.2]], type=float, \n",
    "                        help='The initial value of the filter threshold for state1 or state3')\n",
    "    parser.add_argument('--RL_step0', default=0.02, type=float, help='The starting epoch of RL for state1 or state3')\n",
    "    parser.add_argument('--RL-start0', default=0, type=int, help='The starting epoch of RL for state1 or state3')\n",
    "    \n",
    "    # RL-1\n",
    "    parser.add_argument('--eps_start', default=0.001, type=float, help='The initial value of the eps for state2')\n",
    "    parser.add_argument('--eps_step', default=0.02, type=float, help='The step size of eps for state2')\n",
    "    parser.add_argument('--min_Pts_start', default=2, type=int, help='The initial value of the min_Pts for state2')\n",
    "    parser.add_argument('--min_Pts_step', default=1, type=int, help='The step size of min_Pts for state2')\n",
    "    \n",
    "    # other arguments\n",
    "    parser.add_argument('--use_cuda', dest='use_cuda', default=True, action='store_true', help='Use cuda')\n",
    "    parser.add_argument('--data_path', default='./incremental_0502/', type=str, help='Path of features, labels and edges')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--mask_path', default=None, type=str, help='File path that contains the training, validation and test masks')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--log_interval', default=10, type=int, help='Log interval')\n",
    "    \n",
    "    args = parser.parse_args()  # 解析参数\n",
    "    \n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "697c8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_stage(train_i, i,\n",
    "                 args,\n",
    "                 metrics,\n",
    "                 embedding_save_path,\n",
    "                 loss_fn,\n",
    "                 model=None,\n",
    "                 loss_fn_dgi=None):\n",
    "    # step1: make dir for graph i\n",
    "    # ./incremental_0808//embeddings_0403005348/block_xxx\n",
    "    save_path_i = embedding_save_path + '/block_' + str(i)\n",
    "    if not os.path.isdir(save_path_i):\n",
    "        os.mkdir(save_path_i)\n",
    "    \n",
    "    # step2: load data\n",
    "    realtion_ids: List[str] = ['entity', 'userid', 'word']\n",
    "    homo_data = create_offline_homodataset(args.data_path, [train_i, i])\n",
    "    multi_r_data = create_multi_relational_graph(args.data_path, relation_ids, [train_i, i])\n",
    "    num_relations = len(multi_r_data)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_cuda else 'cpu')\n",
    "    \n",
    "    # input dimension (300 in our paper)\n",
    "    num_dim = homo_data.x.size(0)\n",
    "    feat_dim = homo_data.x.size(1)\n",
    "    \n",
    "    # prepare graph configs for node filtering\n",
    "    if args.is_initial:\n",
    "        print('prepare node configures...')\n",
    "        pre_node_dist(multi_r_data, homo_data.x, save_path_i)\n",
    "        filter_path = save_path_i\n",
    "    else:\n",
    "        filter_path = args.data_path + str(i)\n",
    "    \n",
    "    if model is None:  # pre-training stage in our paper\n",
    "        # print('Pre-Train Stage...')\n",
    "        model = MarGNN((feat_dim, args.hidden_dim, args.out_dim, args.heads), \n",
    "                      num_relations=num_relations, inter_opt=args.inter_opt,is_shared=args.is_shared)\n",
    "    \n",
    "    # define sampler\n",
    "    sampler = MySampler(args.sampler)\n",
    "    # load model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialize RL thresholds\n",
    "    # RL_threshold: [[.5],[.5],[.5]]\n",
    "    RL_thresholds = torch.FloatTensor(args.threhold_start0)\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    \n",
    "    # record training log\n",
    "    message = '\\n------Start initial training /maintaining using block' + str(i) + '------\\n'\n",
    "    print(message)\n",
    "    with open(save_path_i + '/log.txt', 'a') as f:\n",
    "        f.write(message)\n",
    "    \n",
    "    # step12.0: record the highest validation nmi ever got for early stopping\n",
    "    best_vali_nmi = 1e-9\n",
    "    best_epoch = 0\n",
    "    wait = 0\n",
    "    # step12.1: record validation nmi of all epochs before early stop\n",
    "    all_vali_nmi = []\n",
    "    # step12.2: record the time spent in seconds on each batch of all training/maintaining epochs\n",
    "    seconds_train_batches = []\n",
    "    # step12.3: record the time spent in mins on each epoch\n",
    "    mins_train_epochs = []\n",
    "    \n",
    "    # step13: start training \n",
    "    for epoch in range(args.n_epochs):\n",
    "        start_epoch = time.time()\n",
    "        losses = []\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "        \n",
    "        # Multi-Agent\n",
    "        \n",
    "        # filter neighbor in adbvance to fit with neighbor sampling\n",
    "        filtered_multi_r_data = RL_neighbor_filter(multi_r_data, RL_thresholds, filter_path) if epoch >= args.RL_start0 and \\\n",
    "                                args.sampler == 'RL_sampler' else multi_r_data\n",
    "        # step13.0: forward\n",
    "        model.train()\n",
    "        \n",
    "        train_num_samples, valid_num_samples, test_num_samples = homo_data.train_mask.size(0), homo.val_mask.size(0), homo_data.test_mask.size(0)\n",
    "        all_num_samples = train_num_samples + valid_num_samples + test_num_samples\n",
    "        \n",
    "        torch.save(homo_data.train_mask, save_path_i + '/train_mask.pt')\n",
    "        torch.save(homo_data.val_mask, save_path_i + '/valid_mask.pt')\n",
    "        torch.save(homo_data.test_mask, save_path_i + '/test_mask.pt')\n",
    "        \n",
    "        # batch training\n",
    "        num_batches = int(train_num_samples / args.batch_size) + 1\n",
    "        for batch in range(num_batches):\n",
    "            start_batch = time.time()\n",
    "            # split batch\n",
    "            i_start = args.batch_size * batch\n",
    "            i_end = min((batch + 1) * args.batch_size, train_num_samples)\n",
    "            batch_nodes = homo_data.train_mask[i_start:i_end]\n",
    "            batch_labels = homo_data.y[batch_nodes]\n",
    "            \n",
    "            # sampling neighbors of batch nodes\n",
    "            adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, size=[-1,-1],batch_size=args.batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()  # 将参数置0\n",
    "            \n",
    "            pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n",
    "            \n",
    "            loss_outputs = loss_fn(pred, batch_labels)\n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            losses.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # step13.1: metrics\n",
    "            for metric in metrics:\n",
    "                metric(pred, batch_labels, loss_outputs)\n",
    "            if batch % args.log_interval == 0:\n",
    "                message = 'Train: [{}/{} ({:.0f}%)] \\tloss: {:.6f}'.format(batch * args.batch_size, train_num_samples,\n",
    "                          100. * batch / ((train_num_samples // args.batch_size) + 1), np.mean(losses))\n",
    "                for metric in metrics:\n",
    "                    message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n",
    "                # print(message)\n",
    "                with open(save_path_i + '.log.txt', 'a') as f:\n",
    "                    f.write(message)\n",
    "                losses = []\n",
    "            \n",
    "            # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "            del pred, loss_outputs\n",
    "            gc.collect()\n",
    "            \n",
    "            # step13.2: backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_seconds_spent = time.time() - start_batch\n",
    "            seconds_train_batches.append(batch_seconds_spent)\n",
    "            \n",
    "            del loss\n",
    "            gc.collect()\n",
    "        \n",
    "        # step14: print loss\n",
    "        total_loss /= (batch + 1)\n",
    "        message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch, args.n_epochs, total_loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n",
    "        mins_spent = (time.time() - start_epoch) / 60\n",
    "        message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n",
    "        message += '\\n'\n",
    "        print(message)\n",
    "        with open(save_path_i + '/log.txt', 'a') as f:\n",
    "            f.write(message)\n",
    "        mins_train_epochs.append(mins_spent)\n",
    "        \n",
    "        # step15: validation\n",
    "        # inder the representations of all tweets\n",
    "        model.eval()\n",
    "        \n",
    "        # we recommend to forward all nodes and select the validation indices instead\n",
    "        extract_features = torch.FloatTensor([])\n",
    "        num_batches = int(all_num_samples / args.batch_size) + 1\n",
    "        \n",
    "        # all mask are then splited into mini-batch in order\n",
    "        all_mask = torch.arange(0, num_dim, dtype=torch.long)\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            start_batch = time.time()\n",
    "            \n",
    "            # split batch\n",
    "            i_start = args.batch_size * batch\n",
    "            i_end = min((batch + 1) * args.batch_size, all_num_samples)\n",
    "            batch_nodes = all_mask[i_start:i_end]\n",
    "            \n",
    "            # sampling neighbors of batch nodes\n",
    "            adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1,-1], batch_size=args.batch_size)\n",
    "            \n",
    "            pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n",
    "            \n",
    "            extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n",
    "            \n",
    "            del pred\n",
    "            gc.collect()\n",
    "        \n",
    "        # evaluate the model: conduct kMeans clustering on the validation and report NMI\n",
    "        validation_nmi = evaluate(extract_features[homo_data.val_mask],\n",
    "                                 homo_data.y,\n",
    "                                 indices=homo_data.val_mask,\n",
    "                                 epoch=epoch,\n",
    "                                 num_isolated_nodes=0,\n",
    "                                 save_path=save_path_i,\n",
    "                                 is_validation=True,\n",
    "                                 cluster_type=args.cluster_type)\n",
    "        all_vali_nmi.append(validation_nmi)\n",
    "        \n",
    "        # step16: early stop\n",
    "        if validation_nmi > best_vali_nmi:\n",
    "            best_vali_nmi = validation_nmi\n",
    "            best_epoch = epoch\n",
    "            wait = 0\n",
    "            # save model\n",
    "            model_path = save_path_i + '/models'\n",
    "            if (epoch == 0) and (not os.path.isdir(model_path)):\n",
    "                os.mkdir(model_path)\n",
    "            p = model_path + '/best.pt'\n",
    "            torch.save(model.stage_dict(), p)\n",
    "            print('Best model was at epoch ', str(best_epoch))\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= args.patience:\n",
    "            print('Saved all_mins_spent')\n",
    "            print('Early stopping at epoch ', str(epoch))\n",
    "            print('Best model was at epoch ', str(best_epoch))\n",
    "            break\n",
    "        # end one epoch\n",
    "    \n",
    "    # step17: save all validation nmi\n",
    "    np.save(save_path_i + '/all_vali_nmi.npy', np.asarray(all_vali_nmi))\n",
    "    # save time spent on epochs\n",
    "    np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n",
    "    print('Saved mins_train_epochs.')\n",
    "    # save time spent on batches\n",
    "    np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n",
    "    print('Saved seconds_train_batches.')\n",
    "    \n",
    "    # step18: load the best model of the current block\n",
    "    best_model_path = save_path_i + '/models/best.pt'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print('Best model loaded.')\n",
    "    \n",
    "    # del homo_data, multi_r_data\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    \n",
    "    # we recommend to forward all nodes and select the validation indices instead\n",
    "    extract_features = torch.FloatTensor([])\n",
    "    num_batches = int(all_num_samples /args.batch_size) + 1\n",
    "    \n",
    "    # all mask are then splited into mini-batch in order\n",
    "    all_mask = torch.arange(0, num_dim, dtype= torch.long)\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        start_batch = time.time()\n",
    "        \n",
    "        # split batch\n",
    "        i_start = args.batch_size * batch\n",
    "        i_end = min((batch +1) * args.batch_size, all_num_samples)\n",
    "        batch_nodes = all_mask[i_start:i_end]\n",
    "        batch_labels = homo_data.y[batch_nodes]\n",
    "        \n",
    "        # sampling neighbors of batch nodes\n",
    "        adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1,-1], batch_size=args.batch_size)\n",
    "        \n",
    "        pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n",
    "        \n",
    "        extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n",
    "        del pred\n",
    "        gc.collect()\n",
    "        \n",
    "    save_embeddings(extract_features, save_path_i)\n",
    "    \n",
    "    test_nmi = evaluate(extract_features[homo_data.test_mask],\n",
    "                       homo_data.y,\n",
    "                       indices=homo_data.test_mask,\n",
    "                       epoch=-1,\n",
    "                       num_isolated_nodes=0,\n",
    "                       save_path=save_path_i,\n",
    "                       is_validation=True,\n",
    "                       cluster_type=args.cluster_type)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a61f5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_epochs N_EPOCHS] [--window_size WINDOW_SIZE] [--patience PATIENCE]\n",
      "                             [--margin MARGIN] [--lr LR] [--batch_size BATCH_SIZE] [--hidden_dim HIDDEN_DIM]\n",
      "                             [--out_dim OUT_DIM] [--heads HEADS] [--validation_percent VALIDATION_PERCENT]\n",
      "                             [--use_hardest_neg] [--is_shared IS_SHARED] [--inter_opt INTER_OPT]\n",
      "                             [--is_initial IS_INITIAL] [--sampler SAMPLER] [--cluster_type CLUSTER_TYPE]\n",
      "                             [--threshold_start0 THRESHOLD_START0] [--RL_step0 RL_STEP0] [--RL-start0 RL_START0]\n",
      "                             [--eps_start EPS_START] [--eps_step EPS_STEP] [--min_Pts_start MIN_PTS_START]\n",
      "                             [--min_Pts_step MIN_PTS_STEP] [--use_cuda] [--data_path DATA_PATH]\n",
      "                             [--mask_path MASK_PATH] [--log_interval LOG_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\yysgz\\AppData\\Roaming\\jupyter\\runtime\\kernel-179d08b2-6a7c-4674-b3ab-198f3dcbaecf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # define args\n",
    "    args = args_register()\n",
    "    \n",
    "    # check CUDA\n",
    "    print('Using CUDA:', torch.cuda.is_available())\n",
    "    \n",
    "    # create working path\n",
    "    embedding_save_path = args.data_path + '/embeddings_' + strftime('%m%d%H%M%S', localtime())\n",
    "    os.mkdir(embedding_save_path)\n",
    "    print('embedding save path: ', embedding_save_path)\n",
    "    \n",
    "    # record hyper-parameters\n",
    "    with open(embedding_save_path + '/args.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "    \n",
    "    print('Batch Size:', args.batch_size)\n",
    "    print('Intra Agg Mode:', args.is_shared)\n",
    "    print('Inter Agg Mode:', args.inter_opt)\n",
    "    print('Reserve node config?', args.is_initial)\n",
    "    # load number of message in each blocks\n",
    "    # e.g. data_split = [  500  ,   100, ...,  100]\n",
    "    #                    block_0  block_1    block_n\n",
    "    \n",
    "    # define loss function\n",
    "    # contrastive loss in our paper \n",
    "    if args.use_hardest_neg:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, HardestNegativeTripletSelector(args.margin))\n",
    "    else:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, RandomNegativeTripletSelector(args.margin))\n",
    "    \n",
    "    # define metrics\n",
    "    BCL_metrics = [AverageNonzeroTripletsMetric()]\n",
    "    \n",
    "    # define detection stage\n",
    "    Streaming = FinEvent(args)\n",
    "    \n",
    "    # pre-train stage: train on initial graph\n",
    "    train_i = 0\n",
    "    model, RL_thresholds = offline_stage(train_i=train_i,\n",
    "                                        args=args,\n",
    "                                        i=0,\n",
    "                                        metrics=BCL_metrics,\n",
    "                                        embedding_save_path=embedding_save_path,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee907c5",
   "metadata": {},
   "source": [
    "# FinEvent Cross-lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8e4c0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f7b8d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_register():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_epochs', default=50, type=int, help='Number of initial-training/maintenance-training epochs.')\n",
    "    parser.add_argument('--window_size', default=3, type=int, help='Maintain the model after predicting window_size blocks.')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='Early stop if performance did not improve in the last patience epochs.')\n",
    "    parser.add_argument('--margin', default=3., type=float, help='Margin for computing triplet losses')\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='Learning rate')\n",
    "    \n",
    "    parser.add_argument('--batch_size', default=100, type=int, help='Batch size (number of nodes sampled to compute triplet loss in each batch)')\n",
    "    parser.add_argument('--hidden_dim', default=128, type=int, help='Hidden dimension')\n",
    "    parser.add_argument('--out_dim', default=64, type=int, help='Output dimension of tweet representations')\n",
    "    parser.add_argument('--heads', default=4, type=int, help='Number of heads used in GAT')\n",
    "    parser.add_argument('--validation_percent', default=0.2, type=float, help='Percentage of validation nodes(tweets)')\n",
    "    \n",
    "    parser.add_argument('--use_hardest_neg', dest='use_hardest_neg', default=False, action='store_true', \n",
    "                       help='If true, use hardest negative messages to form triplets. Otherwise use random ones')\n",
    "    parser.add_argument('--is_shared', default=False)\n",
    "    parser.add_argument('--inter_opt', default='cat_w_avg')\n",
    "    parser.add_argument('--is_initial', default=False)\n",
    "    parser.add_argument('--sampler', default='RL_sampler')\n",
    "    parser.add_argument('--cluster_type', default='kmeans', help='Types of clustering algorithms')  # DBSCAN\n",
    "    \n",
    "    # RL-0\n",
    "    parser.add_argument('--threshold_start0', default=[[0.5],[0.5],[0.5]], type=float, \n",
    "                        help='The initial value of the filter threshold for state1 or state3')\n",
    "    parser.add_argument('--RL_step0', default=0.02, type=float, help='The step size of RL for state1 or state3')\n",
    "    parser.add_argument('--RL_start0', default=0, type=int, help='The starting epoch of RL for state1 or state3')\n",
    "    \n",
    "    # RL-1\n",
    "    parser.add_argument('--eps_start', default=0.001, type=float, help='The initial value of the eps for state2')\n",
    "    parser.add_argument('--eps_step', default=0.02, type=float, help='The step size of eps for state2')\n",
    "    parser.add_argument('--min_Pts_start', default=2, type=int, help='The initial value of the min_Pts for state2')\n",
    "    parser.add_argument('--min_Pts_step', default=1, type=int, help='The step size of min_Pts for state2')\n",
    "    \n",
    "    # other arguments\n",
    "    parser.add_argument('--use_cuda', dest='use_cuda', default=True, action='store_true', help='Use cuda')\n",
    "    parser.add_argument('--data_path', default='./incremental_0502/', type=str, help='Path of features, labels and edges')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--mask_path', default=None, type=str, help='File path that contains the training, validation and test masks')\n",
    "    # format: './incremental_0808/incremental_graphs_0808/embeddings_XXXX'\n",
    "    parser.add_argument('--resume_path', default='incremental_cross_English_68841/', type=str, \n",
    "                       help='Resume trained model and directly used to inference')\n",
    "    parser.add_argument('--log_interval', default=10, type=int, help='Log interval')\n",
    "    args = parser.parse_args()  # 解析参数\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "32cb678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_epochs N_EPOCHS] [--window_size WINDOW_SIZE] [--patience PATIENCE]\n",
      "                             [--margin MARGIN] [--lr LR] [--batch_size BATCH_SIZE] [--hidden_dim HIDDEN_DIM]\n",
      "                             [--out_dim OUT_DIM] [--heads HEADS] [--validation_percent VALIDATION_PERCENT]\n",
      "                             [--use_hardest_neg] [--is_shared IS_SHARED] [--inter_opt INTER_OPT]\n",
      "                             [--is_initial IS_INITIAL] [--sampler SAMPLER] [--cluster_type CLUSTER_TYPE]\n",
      "                             [--threshold_start0 THRESHOLD_START0] [--RL_step0 RL_STEP0] [--RL_start0 RL_START0]\n",
      "                             [--eps_start EPS_START] [--eps_step EPS_STEP] [--min_Pts_start MIN_PTS_START]\n",
      "                             [--min_Pts_step MIN_PTS_STEP] [--use_cuda] [--data_path DATA_PATH]\n",
      "                             [--mask_path MASK_PATH] [--resume_path RESUME_PATH] [--log_interval LOG_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\yysgz\\AppData\\Roaming\\jupyter\\runtime\\kernel-179d08b2-6a7c-4674-b3ab-198f3dcbaecf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # define args\n",
    "    args = args_register()\n",
    "    \n",
    "    # check CUDA\n",
    "    print('Using CUDA:', torch.cuda.is_available())\n",
    "    \n",
    "    # create working path\n",
    "    embedding_save_path = args.data_path + '/embeddings_' + strftime('%m%d%H%M%S', localtime())\n",
    "    os.mkdir(embedding_save_path)\n",
    "    print('embedding save path:', embedding_save_path)\n",
    "    \n",
    "    print('Batch size: ', args.batch_size)\n",
    "    print('Intra Agg Mode: ', args.is_shared)\n",
    "    print('Inter Agg Mode: ', args.inter_opt)\n",
    "    print('Reserve node config?', args.is_initial)\n",
    "    \n",
    "    print('Trained model from %s dataset' % args.resume_path)\n",
    "    print('Inference dataset: ', args.data_path)\n",
    "    \n",
    "    # record hyper=parameters\n",
    "    with open(embedding_save_path + '/args.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "        \n",
    "    # load number of messages in each blocks\n",
    "    # e.g. data_split = [  500  ,   100, ...,  100]\n",
    "    #                    block_0  block_1    block_n\n",
    "    data_split = np.load(args.data_path + '/data_split.npy')\n",
    "    # define loss function\n",
    "    # contrastive loss in our paper\n",
    "    if args.use_hardest_neg:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, HardestNegativeTripletSelector(args.margin))\n",
    "    else:\n",
    "        loss_fn = OnlineTripletLoss(args.margin, RandomNegativeTripletSelector(args.margin))\n",
    "    \n",
    "    # define metrics\n",
    "    BCL_metrics = [AverageNonzeroTripletsMetric()]\n",
    "    \n",
    "    # define detection stage\n",
    "    Streaming = FinEvent(args)\n",
    "    \n",
    "    # pre-train stage: train on initial graph\n",
    "    train_i = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MarGNN((302, args.hidden_dim, args.out_dim, args.heads), \n",
    "                   num_relations=3, inter_opt=args.inter_opt, is_shared=args.is_shared)\n",
    "    model.load_state_dict(torch.laod(args.resume_path + '/block_18/models/best.pt'))\n",
    "    model, RL_thresholds = Streaming.initial_maintain(train_i=train_i,\n",
    "                                                     i=0,\n",
    "                                                     metrics=BCL_metrics,\n",
    "                                                     embedding_save_path=embedding_save_path,\n",
    "                                                     loss_fn=loss_fn,\n",
    "                                                     model=None)\n",
    "    \n",
    "    # detection-maintenance stage: incremental training and detection\n",
    "    for i in range(1, data_split.shape[0]):\n",
    "        # infer every block\n",
    "        model = Streaming.inference(train_i=train_i,\n",
    "                                   i=i,\n",
    "                                   metrics=BCL_metrics,\n",
    "                                   embedding_save_path=embedding_save_path,\n",
    "                                   loss_fn=loss_fn,\n",
    "                                   model=model,\n",
    "                                   RL_thresholds=RL_thresholds)\n",
    "        # maintenance in window size and desert the last block\n",
    "        # if i % args.window_size == 0 and i != data_split.shape[0] - 1:\n",
    "        #     train_i = i\n",
    "        #     model.load_state_dict(torch.load(args.resume_path + '/block_%d/models/best.pt' % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca38ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "480px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
